{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks using NumPy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Implementation-of-Layers\" data-toc-modified-id=\"Implementation-of-Layers-1\">Implementation of Layers</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fully-Connected-Layer\" data-toc-modified-id=\"Fully-Connected-Layer-1.1\">Fully Connected Layer</a></span></li><li><span><a href=\"#ReLU-and-Softmax-Activation-Functions\" data-toc-modified-id=\"ReLU-and-Softmax-Activation-Functions-1.2\">ReLU and Softmax Activation Functions</a></span></li><li><span><a href=\"#Cross-Entropy-Loss\" data-toc-modified-id=\"Cross-Entropy-Loss-1.3\">Cross Entropy Loss</a></span></li></ul></li><li><span><a href=\"#Image-Classification\" data-toc-modified-id=\"Image-Classification-2\">Image Classification</a></span><ul class=\"toc-item\"><li><span><a href=\"#Data-Preparation-and-Other-Utilities\" data-toc-modified-id=\"Data-Preparation-and-Other-Utilities-2.1\">Data Preparation and Other Utilities</a></span></li><li><span><a href=\"#Two-classes\" data-toc-modified-id=\"Two-classes-2.2\">Two classes</a></span></li><li><span><a href=\"#Five-classes\" data-toc-modified-id=\"Five-classes-2.3\">Five classes</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the required packages\n",
    "import numpy as np\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage.feature import hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .nn.activation import *\n",
    "from .nn.linear import *\n",
    "from .nn.loss import *\n",
    "from .nn.module import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define a class called `Module` which will serve as the base class for definining any other Neural Network modules or layers further on. This, in general, is a good practice since we can define all the necessary functions which are common to all modules that are defined or will be defined. This allows users to define their own modules without re-defining all the common necessary functions. However, in our case, the class `Module` is a dummy class which basically passes the input (in the forward pass) and the gradients (in the backward pass) ahead without modifying them. This is because we re-define all the three basic functions `__init__`, `forward` and `backward` in all the layer classes later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the base class for any layer or module\n",
    "class Module:\n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def forward(self, inputBatch):\n",
    "        return inputBatch\n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        g = np.eye(gradients.shape[1])\n",
    "        return np.dot(gradients, g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `forward` pass, the Fully Connected or the `Linear` layer class applies a linear transformation to the input $\\mathbf{x}$ using a weight matrix $\\mathbf{W}$ and a bias vector $\\mathbf{b}$ as follows:\n",
    "\n",
    "$$ \\mathbf{z} = \\mathbf{x}\\mathbf{W}^T + \\mathbf{b} $$\n",
    "\n",
    "where $\\mathbf{x}$ is a row vector of size $F_{in}$, $\\mathbf{z}$ is a row vector of size $F_{out}$, $\\mathbf{W}$ is a matrix of size $(F_{out}, F_{in})$ and $\\mathbf{b}$ is a row vector of size $F_{out}$. The weight matrix and the bias vector are initialized similarly as in the PyTorch framework.\n",
    "\n",
    "**Note**: In mini-batch gradient descent, the size of the input would be $(N, F_{in})$ and the size of the output will be $(N, F_{out})$ with $N$ being the size of the mini-batch. However, for the purpose of illustration we use single samples.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In the `backward` pass, the `Linear` layer needs to calculate the output derivatives with respect to the weight matrix $\\mathbf{W}$, the bias vector $\\mathbf{b}$ and the input $\\mathbf{x}$. These output derivatives are multiplied to the loss gradients received from the further layers to get the loss gradients with respect to the layer parameters. The layer stores the loss gradients with respect to the weight matrix and the bias vector and uses them later to update the parameters. The loss gradients with respect to the input are passed on to the previous layers. \n",
    "\n",
    "The output derivatives described above are as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{W}} &= \\mathbf{x}^T \\\\[1em]\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{b}} &= \\mathbf{I} \\\\[1em]\n",
    "\\frac{\\partial \\mathbf{z}}{\\partial \\mathbf{x}} &= \\mathbf{W}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is an Identity matrix of size $(F_{out}, F_{out})$.\n",
    "\n",
    "Since the value of the input is required during the backward pass, the layer stores the input in a `cache` variable during the forward pass which is then used during the backward pass.\n",
    "\n",
    "**Note**: In mini-batch gradient descent, the loss gradients are computed and passed on for each sample in the mini-batch. The loss gradients with respect to the layer parameters are averaged over all the samples in the mini-batch and this average loss gradient value is stored for updating the parameters later.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "Once the loss gradients are computed, the layer parameters are updated in the `step` function according to the Gradient Descent update rule:\n",
    "\n",
    "$$ \\Theta = \\Theta -  \\alpha \\frac{\\partial L}{\\partial \\Theta}$$\n",
    "\n",
    "where $\\Theta = \\{\\mathbf{W}, \\mathbf{b}\\}$, $\\alpha$ is the learning rate and $L$ is the loss. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "Also, the `num_params` function gives the total number of trainable parameters of the `Linear` layer. It is the number of elements in the weight matrix $\\mathbf{W}$ i.e. $F_{out} F_{in}$ added to the number of elements in the bias vector $\\mathbf{b}$ i.e. $F_{out}$. \n",
    "\n",
    "In our case, only the `Linear` layer has trainable parameters and therefore the `step` and the `num_param` functions are defined only for the `Linear` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition for the Linear layer class\n",
    "class Linear(Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fully Connected layer that applies a linear transformation to the input using weights and biases.\n",
    "    y = w.x + b\n",
    "    \n",
    "    Shapes:\n",
    "    Input - (N, Fin)\n",
    "    Output - (N, Fout)\n",
    "    In Gradients - (N, Fout)\n",
    "    Out Gradients - (N, Fin)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, inFeatures, outFeatures, learningRate):\n",
    "        super(Linear, self).__init__()\n",
    "        self.inFeatures = inFeatures\n",
    "        self.outFeatures = outFeatures\n",
    "        self.lr = learningRate\n",
    "        \n",
    "        #declaring the weight and bias dictionaries for storing their values and gradients\n",
    "        self.weight = dict()\n",
    "        self.bias = dict()\n",
    "        \n",
    "        #initializing the weight and bias values and gradients\n",
    "        self.weight[\"grad\"] = None\n",
    "        self.bias[\"grad\"] = None\n",
    "        self.weight[\"val\"] = np.random.uniform(-np.sqrt(1/inFeatures), np.sqrt(1/inFeatures), size=(outFeatures, inFeatures))\n",
    "        self.bias[\"val\"] = np.random.uniform(-np.sqrt(1/inFeatures), np.sqrt(1/inFeatures), size=(outFeatures))\n",
    "        \n",
    "        #declaring and initializing the cache dictionary\n",
    "        self.cache = dict()\n",
    "        self.cache[\"input\"] = None\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, inputBatch):\n",
    "        #computing the linear transformation and storing the inputs in the cache\n",
    "        outputBatch = np.dot(inputBatch, self.weight[\"val\"].T) + self.bias[\"val\"]\n",
    "        self.cache[\"input\"] = inputBatch\n",
    "        return outputBatch\n",
    "    \n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        #computing the gradients wrt the weight\n",
    "        [N, Fin] = self.cache[\"input\"].shape\n",
    "        wGrad = np.einsum('no,ni->noi', gradients, self.cache[\"input\"])\n",
    "        self.weight[\"grad\"] = np.mean(wGrad, axis=0)\n",
    "        \n",
    "        #computing the gradients wrt the bias\n",
    "        bGrad = np.dot(gradients, np.eye(gradients.shape[1]))\n",
    "        self.bias[\"grad\"] = np.mean(bGrad, axis=0)\n",
    "        \n",
    "        #computing the gradients wrt the input\n",
    "        inGrad = self.weight[\"val\"]\n",
    "        return np.dot(gradients, inGrad)\n",
    "    \n",
    "    \n",
    "    def step(self):\n",
    "        #weight and bias values update\n",
    "        self.weight[\"val\"] = self.weight[\"val\"] - self.lr*self.weight[\"grad\"]\n",
    "        self.bias[\"val\"] = self.bias[\"val\"] - self.lr*self.bias[\"grad\"]\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def num_params(self):\n",
    "        #total number of trainable parameters in the layer\n",
    "        numParams = (self.weight[\"val\"].shape[0]*self.weight[\"val\"].shape[1]) + self.bias[\"val\"].shape[0]\n",
    "        return numParams"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU and Softmax Activation Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `forward` pass, the `ReLU` layer applies the ReLU operation elementwise on the input $\\mathbf{z}$. The ReLU operation is defined as:\n",
    "\n",
    "$$ \\mathbf{a} = \\text{ReLU}(\\mathbf{z}) = \\max(0,\\mathbf{z}) $$\n",
    "\n",
    "where $\\mathbf{z}$ and $\\mathbf{a}$ are row vectors of size $F$. The ReLU operation clips all the negative values to $0$ while keeping the positive values unmodified. \n",
    "\n",
    "<br><br>\n",
    "\n",
    "In the `backward` pass, the `ReLU` layer calculates the output derivative with respect to the input $\\mathbf{z}$ as follows:  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} &= \\text{diag}\\left(\\frac{\\partial a_0}{\\partial z_0}, \\dots, \\frac{\\partial a_i}{\\partial z_i}, \\dots, \\frac{\\partial a_{F-1}}{\\partial z_{F-1}} \\right)\\\\[1em]\n",
    "\\frac{\\partial a_i}{\\partial z_i} &= \n",
    "\\begin{cases} \n",
    "  0 & \\max(0,z_i) = 0 \\equiv z_i \\leq 0\\\\\n",
    "  1 & \\max(0,z_i) = z_i \\equiv z_i > 0\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since we only need to know whether the ReLU was active ($z_i > 0$) or inactive ($z_i \\leq 0$) at a position $i$ for computing the output derivatives in the backward pass, we store only a boolean map of size $F$ denoting whether the ReLU was active at that position in the `cache` variable.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The `ReLU` layer has no trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition for the ReLU layer class\n",
    "class ReLU(Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies the ReLU function on each element of the input block.\n",
    "    y = max(0,x)\n",
    "    \n",
    "    Shapes:\n",
    "    Input - (N,F)\n",
    "    Output - (N,F)\n",
    "    In Gradients - (N,F)\n",
    "    Out Gradients - (N,F)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        #declaring and initializing the cache dictionary \n",
    "        self.cache = dict()\n",
    "        self.cache[\"active\"] = None\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, inputBatch):\n",
    "        #applying the ReLU operation and storing a map showing where ReLU was active\n",
    "        outputBatch = np.maximum(0, inputBatch)\n",
    "        self.cache[\"active\"] = (inputBatch > 0)\n",
    "        return outputBatch\n",
    "    \n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        #computing the gradients wrt the input\n",
    "        inGrad = np.zeros(self.cache[\"active\"].shape)\n",
    "        inGrad[self.cache[\"active\"]] = 1\n",
    "        return gradients*inGrad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the `forward` pass, the `Softmax` layer applies the Softmax operation on the input $\\mathbf{z}$. The Softmax operation is used to convert the input $\\mathbf{z}$ to a probability distribution so that the elements in $\\mathbf{z}$ sum to $1$ and each element lies between $0$ and $1$. The Softmax operation is defined as:\n",
    "\n",
    "$$ \n",
    "\\begin{align*}\n",
    "\\mathbf{a} &= \\text{Softmax}(\\mathbf{z})\\\\[1em]\n",
    "a_i &= \\frac{e^{z_i}}{\\sum_{j=0}^{F-1} e^{z_j}} \\quad \\forall i \\in \\{0, \\dots, F-1\\}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z}$ and $\\mathbf{a}$ are row vectors of size $F$. \n",
    "\n",
    "If the values of elements of $\\mathbf{z}$ are high, the exponent values may cross the `float` datatype limits and thus we would get `nan` in the softmax output. Thus for numerical stability, we modify the softmax operation along with a correction factor $D$ as follows:\n",
    "\n",
    "$$ a_i = \\frac{e^{z_i - D}}{\\sum_{j=0}^{F-1} e^{z_j - D}} \\quad \\forall i \\in \\{0, \\dots, F-1\\} $$\n",
    "\n",
    "The correction factor $D$ is generally taken to be $\\max(z_0, \\dots, z_{F-1})$. Using the correction factor, we avoid `nan` in output while not changing the actual Softmax output values.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In the `backward` pass, the `Softmax` layer calculates the output derivative with respect to the input $\\mathbf{z}$ as follows:  \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left( \\frac{\\partial \\mathbf{a}}{\\partial \\mathbf{z}} \\right)_{ij} &= \\frac{\\partial a_i}{\\partial z_j} \\quad \\forall i,j \\in \\{ 0,\\dots,F-1 \\}\\\\[1em]\n",
    "\\frac{\\partial a_i}{\\partial z_j} &= a_i(\\delta_{ij} - a_j)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\delta_{ij}$ is the Kroneker Delta defined as:\n",
    "\n",
    "$$ \n",
    "\\delta_{ij} = \n",
    "\\begin{cases} \n",
    "  0 & i \\neq j\\\\\n",
    "  1 & i = j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Since we need the output values for the output derivative computation in backward pass, we store the outputs computed in the forward pass in the `cache` variable which is then used during the backward pass.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The `Softmax` layer has no trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition for the Softmax layer class    \n",
    "class Softmax(Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies the Softmax function along the last axis/dimension of the input batch\n",
    "    yi = exp(xi)/sum_j(exp(xj))\n",
    "    \n",
    "    Shapes:\n",
    "    Input - (N,F)\n",
    "    Output - (N,F)\n",
    "    In Gradients - (N,F)\n",
    "    Out Gradients - (N,F)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        #declaring and initializing the cache dictionary\n",
    "        self.cache = dict()\n",
    "        self.cache[\"output\"] = None\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, inputBatch):\n",
    "        #computing the exponential values along with a correction factor for numerical stability\n",
    "        correction = np.max(inputBatch, axis=1, keepdims=True)\n",
    "        expVals = np.exp(inputBatch - correction)\n",
    "        \n",
    "        #computing the softmax operation\n",
    "        outputBatch = expVals/np.sum(expVals, axis=1, keepdims=True)\n",
    "        \n",
    "        #storing the outputs in the cache\n",
    "        self.cache[\"output\"] = outputBatch\n",
    "        return outputBatch\n",
    "    \n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        #computing the gradients wrt the input\n",
    "        [N,F] = self.cache[\"output\"].shape\n",
    "        delta = np.eye(F,F)\n",
    "        delta = np.reshape(delta, (1,F,F))\n",
    "        output = np.reshape(self.cache[\"output\"], (N,1,F))\n",
    "        inGrad = output.transpose(0,2,1)*(delta - output)\n",
    "        return np.einsum('nf,nfd->nd', gradients, inGrad)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy Loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CrossEntropyLoss` layer is used for implementing the Cross Entropy Loss function. This loss function is generally used in classification tasks with 2 or more classes as the output. In the `forward` pass, the `CrossEntropyLoss` layer computes the loss given the model output probability distribution $\\mathbf{y}$ and the target class $T$. The Cross Entropy Loss function is computed as follows:\n",
    "\n",
    "$$ Loss = -\\log y_T $$\n",
    "\n",
    "where $\\mathbf{y}$ is a row vector of size $C$ and $T \\in \\{0, \\dots, C-1\\}$ with $C$ being the number of classes.\n",
    "\n",
    "**Note**: For a mini-batch, the loss values for each sample is computed and the average loss over all the samples is given as the output loss value.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "In the `backward` pass, the `CrossEntropyLoss` layer calculates the loss gradient with respect to the model output distribution  $\\mathbf{y}$ as follows:  \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\left( \\frac{\\partial \\text{Loss}}{\\partial \\mathbf{y}} \\right)_i &= \\frac{\\partial \\text{Loss}}{\\partial y_i}\\\\[1em]\n",
    "\\frac{\\partial \\text{Loss}}{\\partial y_i} &= \n",
    "\\begin{cases} \n",
    "  0 & i \\neq T\\\\\n",
    "  -\\frac{1}{y_T} & i = T\n",
    "  \\end{cases}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since we need the model output probability of the target class i.e $y_T$ and the target class $T$ for computation of loss gradient in the backward pass, we store them during the forward pass in the `cache` variable and use them during the backward pass.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The `CrossEntropyLoss` layer has no trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definition for the Cross Entropy Loss layer class\n",
    "class CrossEntropyLoss(Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    Computes the cross entropy loss using the output and the required target class.\n",
    "    loss = -log(yHat[class])\n",
    "    \n",
    "    Shapes:\n",
    "    Outputs - (N,C)\n",
    "    Classes - (N)\n",
    "    Out Gradients - (N,C)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "        #declaring and initializing the cache dictionary\n",
    "        self.cache = dict()\n",
    "        self.cache[\"scores\"] = None\n",
    "        self.cache[\"classes\"] = None\n",
    "        self.cache[\"numClasses\"] = None\n",
    "        return\n",
    "    \n",
    "    \n",
    "    def forward(self, outputs, classes):\n",
    "        #computing the loss for each sample in the batch and averaging the loss\n",
    "        scores = outputs[np.arange(outputs.shape[0]), classes]\n",
    "        loss = -np.mean(np.log(scores))\n",
    "        \n",
    "        #storing the scores and classes in cache\n",
    "        self.cache[\"scores\"] = scores\n",
    "        self.cache[\"classes\"] = classes\n",
    "        self.cache[\"numClasses\"] = outputs.shape[1]\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        #computing the loss gradients\n",
    "        N = len(self.cache[\"classes\"])\n",
    "        gradients = np.zeros((N, self.cache[\"numClasses\"]))\n",
    "        gradients[np.arange(N), self.cache[\"classes\"]] = -1./self.cache[\"scores\"]\n",
    "        return gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Other Utilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `CIFAR-10` dataset for our task. `CIFAR-10` dataset consists of $10$ classes with $5000$ training set and $1000$ test set 32 x 32 RGB images for each class. We perform two tasks using our Neural Network: \n",
    "\n",
    "1. Two class classification (`Airplane`(0) / `Ship`(1)) \n",
    "2. Five class classification (`Cat`(0) / `Deer`(1) / `Dog`(2) / `Frog`(3) / `Horse`(4))\n",
    "\n",
    "We extract the complete dataset and convert the images to 32 x 32 grayscale images with pixel intensity values between $0$ and $1$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which takes a pickle file as input and outputs the file contents in form of dictionary\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dic = pickle.load(fo, encoding='bytes')\n",
    "    return dic\n",
    "\n",
    "basePath = \"./CIFAR10/data_batch_\"\n",
    "\n",
    "#reading and storing the training set\n",
    "trainImages = np.empty((0,3072), dtype=np.float32)\n",
    "trainLabels = list()\n",
    "for i in range(1,6):\n",
    "    trainset = unpickle(basePath + str(i))\n",
    "    trainImages = np.vstack((trainImages, trainset[b\"data\"].astype(np.float32)))\n",
    "    trainLabels.extend(trainset[b\"labels\"])\n",
    "trainLabels = np.array(trainLabels)\n",
    "\n",
    "#reading and storing the test set\n",
    "testset = unpickle(\"./CIFAR10/test_batch\")\n",
    "testImages = testset[b\"data\"].astype(np.float32)\n",
    "testLabels = np.array(testset[b\"labels\"])\n",
    "\n",
    "#reading the meta file for the label names\n",
    "labelNames = unpickle(\"./CIFAR10/batches.meta\")[b\"label_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the RGB images to grayscale, scaling them to 0-1 and reshaping them to 32x32\n",
    "trainImages = (0.299*trainImages[:,:1024] + 0.587*trainImages[:,1024:2048] + 0.114*trainImages[:,2048:])/255.0\n",
    "trainImages = trainImages.reshape((trainImages.shape[0], 32, 32))\n",
    "\n",
    "testImages = (0.299*testImages[:,:1024] + 0.587*testImages[:,1024:2048] + 0.114*testImages[:,2048:])/255.0\n",
    "testImages = testImages.reshape((testImages.shape[0], 32, 32))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Steps**\n",
    "\n",
    "For one epoch:\n",
    "1. Divide the training data set into batches depending upon the batch size.\n",
    "2. For each batch, do a forward pass of the model and obtain the model outputs.\n",
    "3. Given the model outputs and the target labels for the batch, compute the loss value.\n",
    "4. Perform a backward pass on the loss function and the model for obtaining the values of loss gradient with respect to the model parameters.\n",
    "5. Perform one step of Gradient Descent update to update the model parameters.\n",
    "6. Repeat from 2. for all batches.\n",
    "\n",
    "`train` - Trains the model for one epoch over the training set as described above. After training the model for one epoch, the function computes the per sample average training set loss value and accuracy.\n",
    "\n",
    "`validation` - The function computes the per sample average validation set loss value and accuracy.\n",
    "\n",
    "`test` - The function computes the per sample average test set loss value and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function for training the model for one epoch\n",
    "def train(model, X, Y, loss_function):\n",
    "    \n",
    "    #calculate the number of batches given the batch size\n",
    "    numBatches = int(np.ceil(len(X)/BATCH_SIZE))\n",
    "    for batch in range(numBatches):\n",
    "        \n",
    "        #extracting the batch\n",
    "        if batch == numBatches-1:\n",
    "            x = X[int(BATCH_SIZE*batch):,:]\n",
    "            y = Y[int(BATCH_SIZE*batch):]\n",
    "        else:\n",
    "            x = X[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1)),:]\n",
    "            y = Y[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1))]\n",
    "        \n",
    "        #computing the model output and loss\n",
    "        outputs = model.forward(x)\n",
    "        loss = loss_function.forward(outputs, y)\n",
    "        \n",
    "        #backpropagation step to calculate the gradients\n",
    "        lossGradients = loss_function.backward()\n",
    "        model.backward(lossGradients)\n",
    "        \n",
    "        #update the parameters\n",
    "        model.step()\n",
    "\n",
    "        \n",
    "    #getting the training set loss and training set accuracy after one epoch of training\n",
    "    trainingLoss = 0\n",
    "    trainingAccuracy = 0\n",
    "    for batch in range(numBatches):\n",
    "        \n",
    "        #extract the batch\n",
    "        if batch == numBatches-1:\n",
    "            x = X[int(BATCH_SIZE*batch):,:]\n",
    "            y = Y[int(BATCH_SIZE*batch):]\n",
    "        else:\n",
    "            x = X[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1)),:]\n",
    "            y = Y[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1))]\n",
    "            \n",
    "        #computing the model outputs and loss\n",
    "        outputs = model.forward(x) \n",
    "        loss = loss_function.forward(outputs, y) \n",
    "        \n",
    "        #accumulating the per batch loss and accuracy\n",
    "        trainingLoss = trainingLoss + loss\n",
    "        trainingAccuracy = trainingAccuracy + (np.count_nonzero(np.equal(np.argmax(outputs, axis=1), y)))/len(outputs)\n",
    "    \n",
    "    #computing the per sample training set loss and accuracy\n",
    "    trainingLoss = trainingLoss/numBatches\n",
    "    trainingAccuracy = trainingAccuracy/numBatches\n",
    "    return trainingLoss, trainingAccuracy\n",
    "\n",
    "\n",
    "\n",
    "#function to calculate the validation set loss and accuracy\n",
    "def validate(model, X, Y, loss_function):\n",
    "    \n",
    "    #calculate the number of batches given the batch size\n",
    "    numBatches = int(np.ceil(len(X)/BATCH_SIZE))\n",
    "    \n",
    "    validationLoss = 0\n",
    "    validationAccuracy = 0\n",
    "    for batch in range(numBatches):\n",
    "        \n",
    "        #extract the batch\n",
    "        if batch == numBatches-1:\n",
    "            x = X[int(BATCH_SIZE*batch):,:]\n",
    "            y = Y[int(BATCH_SIZE*batch):]\n",
    "        else:\n",
    "            x = X[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1)),:]\n",
    "            y = Y[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1))]\n",
    "            \n",
    "        #computing the model outputs and loss\n",
    "        outputs = model.forward(x) \n",
    "        loss = loss_function.forward(outputs, y) \n",
    "        \n",
    "        #accumulating the per batch loss and accuracy\n",
    "        validationLoss = validationLoss + loss\n",
    "        validationAccuracy = validationAccuracy + (np.count_nonzero(np.equal(np.argmax(outputs, axis=1), y)))/len(outputs)\n",
    "    \n",
    "    #getting the per sample loss and accuracy\n",
    "    validationLoss = validationLoss/numBatches\n",
    "    validationAccuracy = validationAccuracy/numBatches\n",
    "    return validationLoss, validationAccuracy\n",
    "\n",
    "\n",
    "\n",
    "#function to calculate the test set loss and accuracy\n",
    "def test(model, X, Y, loss_function):\n",
    "    \n",
    "    #calculating the number of batches given the batch size\n",
    "    numBatches = int(np.ceil(len(X)/BATCH_SIZE))\n",
    "    testLoss = 0\n",
    "    testAccuracy = 0\n",
    "    for batch in range(numBatches):\n",
    "        \n",
    "        #extract the batch\n",
    "        if batch == numBatches-1:\n",
    "            x = X[int(BATCH_SIZE*batch):,:]\n",
    "            y = Y[int(BATCH_SIZE*batch):]\n",
    "        else:\n",
    "            x = X[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1)),:]\n",
    "            y = Y[int(BATCH_SIZE*batch):int(BATCH_SIZE*(batch+1))]\n",
    "            \n",
    "        #computing the model outputs and loss\n",
    "        outputs = model.forward(x) \n",
    "        loss = loss_function.forward(outputs, y) \n",
    "        \n",
    "        #accumulating the per batch loss and accuracy\n",
    "        testLoss = testLoss + loss\n",
    "        testAccuracy = testAccuracy + (np.count_nonzero(np.equal(np.argmax(outputs, axis=1), y)))/len(outputs)\n",
    "    \n",
    "    #obtaining the per sample loss and accuracy\n",
    "    testLoss = testLoss/numBatches\n",
    "    testAccuracy = testAccuracy/numBatches\n",
    "    return testLoss, testAccuracy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data set to include only the `Airplane` and the `Ship` classes. We assign label $0$ to the `Airplane` class and label $1$ to the `Ship` class. We use the Histogram of Oriented Gradients (HOG) Descriptors as feature vectors. We extract $288$ HOG features for each image. We normalize the data using `mean` and `std` and split the training set into training ($80\\%$) and validation ($20\\%$) sets. \n",
    "\n",
    "\n",
    "The Neural Network architecture used consists of $1$ input layer, $3$ hidden layers each with `ReLU` activation and $1$ output layer with `Softmax` activation so that the model output is a probability distribution. We use the `Cross Entropy` Loss function to train the network.\n",
    "\n",
    "The hyperparameter values used are:\n",
    "- Batch Size = $1024$\n",
    "- Epochs = $175$\n",
    "- Learning Rate = $0.01$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Parameters in Model = 217826\n",
      "\n",
      "Training the model .... \n",
      "\n",
      "Epoch: 1, Tr.Loss: 0.691567, Val.Loss: 0.690865, Tr.Acc: 0.498, Val.Acc: 0.506\n",
      "Epoch: 2, Tr.Loss: 0.690939, Val.Loss: 0.690310, Tr.Acc: 0.503, Val.Acc: 0.510\n",
      "Epoch: 3, Tr.Loss: 0.690377, Val.Loss: 0.689751, Tr.Acc: 0.507, Val.Acc: 0.512\n",
      "Epoch: 4, Tr.Loss: 0.689800, Val.Loss: 0.689175, Tr.Acc: 0.512, Val.Acc: 0.514\n",
      "Epoch: 5, Tr.Loss: 0.689219, Val.Loss: 0.688582, Tr.Acc: 0.519, Val.Acc: 0.520\n",
      "Epoch: 6, Tr.Loss: 0.688576, Val.Loss: 0.687971, Tr.Acc: 0.527, Val.Acc: 0.527\n",
      "Epoch: 7, Tr.Loss: 0.687873, Val.Loss: 0.687340, Tr.Acc: 0.538, Val.Acc: 0.538\n",
      "Epoch: 8, Tr.Loss: 0.687165, Val.Loss: 0.686686, Tr.Acc: 0.548, Val.Acc: 0.543\n",
      "Epoch: 9, Tr.Loss: 0.686539, Val.Loss: 0.686011, Tr.Acc: 0.557, Val.Acc: 0.554\n",
      "Epoch: 10, Tr.Loss: 0.685797, Val.Loss: 0.685304, Tr.Acc: 0.572, Val.Acc: 0.567\n",
      "Epoch: 11, Tr.Loss: 0.684984, Val.Loss: 0.684563, Tr.Acc: 0.585, Val.Acc: 0.580\n",
      "Epoch: 12, Tr.Loss: 0.684202, Val.Loss: 0.683792, Tr.Acc: 0.592, Val.Acc: 0.585\n",
      "Epoch: 13, Tr.Loss: 0.683360, Val.Loss: 0.682987, Tr.Acc: 0.605, Val.Acc: 0.597\n",
      "Epoch: 14, Tr.Loss: 0.682459, Val.Loss: 0.682135, Tr.Acc: 0.616, Val.Acc: 0.607\n",
      "Epoch: 15, Tr.Loss: 0.681598, Val.Loss: 0.681230, Tr.Acc: 0.627, Val.Acc: 0.619\n",
      "Epoch: 16, Tr.Loss: 0.680609, Val.Loss: 0.680270, Tr.Acc: 0.638, Val.Acc: 0.628\n",
      "Epoch: 17, Tr.Loss: 0.679665, Val.Loss: 0.679252, Tr.Acc: 0.650, Val.Acc: 0.636\n",
      "Epoch: 18, Tr.Loss: 0.678539, Val.Loss: 0.678173, Tr.Acc: 0.659, Val.Acc: 0.644\n",
      "Epoch: 19, Tr.Loss: 0.677414, Val.Loss: 0.677026, Tr.Acc: 0.666, Val.Acc: 0.652\n",
      "Epoch: 20, Tr.Loss: 0.676168, Val.Loss: 0.675808, Tr.Acc: 0.674, Val.Acc: 0.650\n",
      "Epoch: 21, Tr.Loss: 0.674954, Val.Loss: 0.674520, Tr.Acc: 0.680, Val.Acc: 0.660\n",
      "Epoch: 22, Tr.Loss: 0.673515, Val.Loss: 0.673148, Tr.Acc: 0.683, Val.Acc: 0.663\n",
      "Epoch: 23, Tr.Loss: 0.672089, Val.Loss: 0.671684, Tr.Acc: 0.686, Val.Acc: 0.665\n",
      "Epoch: 24, Tr.Loss: 0.670573, Val.Loss: 0.670138, Tr.Acc: 0.691, Val.Acc: 0.669\n",
      "Epoch: 25, Tr.Loss: 0.668991, Val.Loss: 0.668507, Tr.Acc: 0.694, Val.Acc: 0.677\n",
      "Epoch: 26, Tr.Loss: 0.667250, Val.Loss: 0.666779, Tr.Acc: 0.696, Val.Acc: 0.682\n",
      "Epoch: 27, Tr.Loss: 0.665451, Val.Loss: 0.664951, Tr.Acc: 0.697, Val.Acc: 0.687\n",
      "Epoch: 28, Tr.Loss: 0.663573, Val.Loss: 0.663033, Tr.Acc: 0.699, Val.Acc: 0.687\n",
      "Epoch: 29, Tr.Loss: 0.661597, Val.Loss: 0.661033, Tr.Acc: 0.701, Val.Acc: 0.691\n",
      "Epoch: 30, Tr.Loss: 0.659528, Val.Loss: 0.658946, Tr.Acc: 0.705, Val.Acc: 0.699\n",
      "Epoch: 31, Tr.Loss: 0.657264, Val.Loss: 0.656746, Tr.Acc: 0.708, Val.Acc: 0.702\n",
      "Epoch: 32, Tr.Loss: 0.655141, Val.Loss: 0.654456, Tr.Acc: 0.710, Val.Acc: 0.702\n",
      "Epoch: 33, Tr.Loss: 0.652743, Val.Loss: 0.652070, Tr.Acc: 0.714, Val.Acc: 0.703\n",
      "Epoch: 34, Tr.Loss: 0.650275, Val.Loss: 0.649583, Tr.Acc: 0.717, Val.Acc: 0.709\n",
      "Epoch: 35, Tr.Loss: 0.647714, Val.Loss: 0.646989, Tr.Acc: 0.720, Val.Acc: 0.714\n",
      "Epoch: 36, Tr.Loss: 0.645017, Val.Loss: 0.644284, Tr.Acc: 0.722, Val.Acc: 0.720\n",
      "Epoch: 37, Tr.Loss: 0.642426, Val.Loss: 0.641477, Tr.Acc: 0.727, Val.Acc: 0.721\n",
      "Epoch: 38, Tr.Loss: 0.639362, Val.Loss: 0.638551, Tr.Acc: 0.732, Val.Acc: 0.726\n",
      "Epoch: 39, Tr.Loss: 0.636364, Val.Loss: 0.635510, Tr.Acc: 0.734, Val.Acc: 0.728\n",
      "Epoch: 40, Tr.Loss: 0.633374, Val.Loss: 0.632352, Tr.Acc: 0.738, Val.Acc: 0.735\n",
      "Epoch: 41, Tr.Loss: 0.630081, Val.Loss: 0.629073, Tr.Acc: 0.739, Val.Acc: 0.736\n",
      "Epoch: 42, Tr.Loss: 0.626658, Val.Loss: 0.625668, Tr.Acc: 0.743, Val.Acc: 0.738\n",
      "Epoch: 43, Tr.Loss: 0.623435, Val.Loss: 0.622161, Tr.Acc: 0.745, Val.Acc: 0.741\n",
      "Epoch: 44, Tr.Loss: 0.619621, Val.Loss: 0.618522, Tr.Acc: 0.749, Val.Acc: 0.743\n",
      "Epoch: 45, Tr.Loss: 0.616084, Val.Loss: 0.614779, Tr.Acc: 0.752, Val.Acc: 0.747\n",
      "Epoch: 46, Tr.Loss: 0.612195, Val.Loss: 0.610918, Tr.Acc: 0.753, Val.Acc: 0.749\n",
      "Epoch: 47, Tr.Loss: 0.608349, Val.Loss: 0.606948, Tr.Acc: 0.756, Val.Acc: 0.754\n",
      "Epoch: 48, Tr.Loss: 0.604249, Val.Loss: 0.602868, Tr.Acc: 0.758, Val.Acc: 0.756\n",
      "Epoch: 49, Tr.Loss: 0.600282, Val.Loss: 0.598702, Tr.Acc: 0.758, Val.Acc: 0.757\n",
      "Epoch: 50, Tr.Loss: 0.596218, Val.Loss: 0.594452, Tr.Acc: 0.760, Val.Acc: 0.760\n",
      "Epoch: 51, Tr.Loss: 0.592017, Val.Loss: 0.590118, Tr.Acc: 0.762, Val.Acc: 0.765\n",
      "Epoch: 52, Tr.Loss: 0.587527, Val.Loss: 0.585708, Tr.Acc: 0.765, Val.Acc: 0.765\n",
      "Epoch: 53, Tr.Loss: 0.583433, Val.Loss: 0.581246, Tr.Acc: 0.767, Val.Acc: 0.767\n",
      "Epoch: 54, Tr.Loss: 0.578923, Val.Loss: 0.576750, Tr.Acc: 0.767, Val.Acc: 0.770\n",
      "Epoch: 55, Tr.Loss: 0.574468, Val.Loss: 0.572215, Tr.Acc: 0.768, Val.Acc: 0.771\n",
      "Epoch: 56, Tr.Loss: 0.569889, Val.Loss: 0.567654, Tr.Acc: 0.769, Val.Acc: 0.772\n",
      "Epoch: 57, Tr.Loss: 0.565532, Val.Loss: 0.563085, Tr.Acc: 0.770, Val.Acc: 0.779\n",
      "Epoch: 58, Tr.Loss: 0.560983, Val.Loss: 0.558507, Tr.Acc: 0.771, Val.Acc: 0.783\n",
      "Epoch: 59, Tr.Loss: 0.556761, Val.Loss: 0.553958, Tr.Acc: 0.773, Val.Acc: 0.784\n",
      "Epoch: 60, Tr.Loss: 0.552233, Val.Loss: 0.549429, Tr.Acc: 0.773, Val.Acc: 0.786\n",
      "Epoch: 61, Tr.Loss: 0.547924, Val.Loss: 0.544948, Tr.Acc: 0.775, Val.Acc: 0.787\n",
      "Epoch: 62, Tr.Loss: 0.543580, Val.Loss: 0.540510, Tr.Acc: 0.776, Val.Acc: 0.788\n",
      "Epoch: 63, Tr.Loss: 0.538693, Val.Loss: 0.536108, Tr.Acc: 0.777, Val.Acc: 0.790\n",
      "Epoch: 64, Tr.Loss: 0.534846, Val.Loss: 0.531767, Tr.Acc: 0.778, Val.Acc: 0.790\n",
      "Epoch: 65, Tr.Loss: 0.530699, Val.Loss: 0.527511, Tr.Acc: 0.779, Val.Acc: 0.792\n",
      "Epoch: 66, Tr.Loss: 0.526275, Val.Loss: 0.523307, Tr.Acc: 0.780, Val.Acc: 0.790\n",
      "Epoch: 67, Tr.Loss: 0.522231, Val.Loss: 0.519184, Tr.Acc: 0.781, Val.Acc: 0.791\n",
      "Epoch: 68, Tr.Loss: 0.518331, Val.Loss: 0.515160, Tr.Acc: 0.781, Val.Acc: 0.791\n",
      "Epoch: 69, Tr.Loss: 0.514515, Val.Loss: 0.511229, Tr.Acc: 0.782, Val.Acc: 0.792\n",
      "Epoch: 70, Tr.Loss: 0.510521, Val.Loss: 0.507374, Tr.Acc: 0.782, Val.Acc: 0.793\n",
      "Epoch: 71, Tr.Loss: 0.507041, Val.Loss: 0.503650, Tr.Acc: 0.784, Val.Acc: 0.796\n",
      "Epoch: 72, Tr.Loss: 0.503947, Val.Loss: 0.500013, Tr.Acc: 0.785, Val.Acc: 0.796\n",
      "Epoch: 73, Tr.Loss: 0.500039, Val.Loss: 0.496492, Tr.Acc: 0.787, Val.Acc: 0.796\n",
      "Epoch: 74, Tr.Loss: 0.496663, Val.Loss: 0.493093, Tr.Acc: 0.787, Val.Acc: 0.799\n",
      "Epoch: 75, Tr.Loss: 0.493380, Val.Loss: 0.489763, Tr.Acc: 0.789, Val.Acc: 0.800\n",
      "Epoch: 76, Tr.Loss: 0.490718, Val.Loss: 0.486527, Tr.Acc: 0.789, Val.Acc: 0.799\n",
      "Epoch: 77, Tr.Loss: 0.487586, Val.Loss: 0.483429, Tr.Acc: 0.790, Val.Acc: 0.800\n",
      "Epoch: 78, Tr.Loss: 0.483684, Val.Loss: 0.480391, Tr.Acc: 0.791, Val.Acc: 0.800\n",
      "Epoch: 79, Tr.Loss: 0.481071, Val.Loss: 0.477478, Tr.Acc: 0.792, Val.Acc: 0.801\n",
      "Epoch: 80, Tr.Loss: 0.478170, Val.Loss: 0.474621, Tr.Acc: 0.792, Val.Acc: 0.800\n",
      "Epoch: 81, Tr.Loss: 0.475446, Val.Loss: 0.471894, Tr.Acc: 0.794, Val.Acc: 0.800\n",
      "Epoch: 82, Tr.Loss: 0.472599, Val.Loss: 0.469227, Tr.Acc: 0.795, Val.Acc: 0.800\n",
      "Epoch: 83, Tr.Loss: 0.469641, Val.Loss: 0.466659, Tr.Acc: 0.796, Val.Acc: 0.800\n",
      "Epoch: 84, Tr.Loss: 0.467226, Val.Loss: 0.464157, Tr.Acc: 0.796, Val.Acc: 0.801\n",
      "Epoch: 85, Tr.Loss: 0.464962, Val.Loss: 0.461725, Tr.Acc: 0.796, Val.Acc: 0.800\n",
      "Epoch: 86, Tr.Loss: 0.462668, Val.Loss: 0.459432, Tr.Acc: 0.797, Val.Acc: 0.801\n",
      "Epoch: 87, Tr.Loss: 0.459894, Val.Loss: 0.457175, Tr.Acc: 0.798, Val.Acc: 0.802\n",
      "Epoch: 88, Tr.Loss: 0.458189, Val.Loss: 0.454978, Tr.Acc: 0.799, Val.Acc: 0.804\n",
      "Epoch: 89, Tr.Loss: 0.455451, Val.Loss: 0.452873, Tr.Acc: 0.800, Val.Acc: 0.803\n",
      "Epoch: 90, Tr.Loss: 0.453311, Val.Loss: 0.450782, Tr.Acc: 0.802, Val.Acc: 0.803\n",
      "Epoch: 91, Tr.Loss: 0.450907, Val.Loss: 0.448771, Tr.Acc: 0.803, Val.Acc: 0.804\n",
      "Epoch: 92, Tr.Loss: 0.449432, Val.Loss: 0.446802, Tr.Acc: 0.802, Val.Acc: 0.805\n",
      "Epoch: 93, Tr.Loss: 0.447209, Val.Loss: 0.444858, Tr.Acc: 0.803, Val.Acc: 0.805\n",
      "Epoch: 94, Tr.Loss: 0.445388, Val.Loss: 0.443020, Tr.Acc: 0.804, Val.Acc: 0.806\n",
      "Epoch: 95, Tr.Loss: 0.442540, Val.Loss: 0.441288, Tr.Acc: 0.805, Val.Acc: 0.806\n",
      "Epoch: 96, Tr.Loss: 0.441874, Val.Loss: 0.439512, Tr.Acc: 0.805, Val.Acc: 0.807\n",
      "Epoch: 97, Tr.Loss: 0.439165, Val.Loss: 0.437797, Tr.Acc: 0.806, Val.Acc: 0.807\n",
      "Epoch: 98, Tr.Loss: 0.436907, Val.Loss: 0.436206, Tr.Acc: 0.806, Val.Acc: 0.807\n",
      "Epoch: 99, Tr.Loss: 0.435377, Val.Loss: 0.434603, Tr.Acc: 0.807, Val.Acc: 0.808\n",
      "Epoch: 100, Tr.Loss: 0.433627, Val.Loss: 0.433004, Tr.Acc: 0.808, Val.Acc: 0.809\n",
      "Epoch: 101, Tr.Loss: 0.432242, Val.Loss: 0.431461, Tr.Acc: 0.808, Val.Acc: 0.809\n",
      "Epoch: 102, Tr.Loss: 0.430145, Val.Loss: 0.430022, Tr.Acc: 0.809, Val.Acc: 0.810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103, Tr.Loss: 0.428149, Val.Loss: 0.428502, Tr.Acc: 0.810, Val.Acc: 0.812\n",
      "Epoch: 104, Tr.Loss: 0.426096, Val.Loss: 0.427058, Tr.Acc: 0.811, Val.Acc: 0.812\n",
      "Epoch: 105, Tr.Loss: 0.424767, Val.Loss: 0.425595, Tr.Acc: 0.812, Val.Acc: 0.814\n",
      "Epoch: 106, Tr.Loss: 0.422965, Val.Loss: 0.424400, Tr.Acc: 0.812, Val.Acc: 0.815\n",
      "Epoch: 107, Tr.Loss: 0.421296, Val.Loss: 0.423048, Tr.Acc: 0.813, Val.Acc: 0.816\n",
      "Epoch: 108, Tr.Loss: 0.420148, Val.Loss: 0.421562, Tr.Acc: 0.814, Val.Acc: 0.816\n",
      "Epoch: 109, Tr.Loss: 0.418236, Val.Loss: 0.420401, Tr.Acc: 0.815, Val.Acc: 0.817\n",
      "Epoch: 110, Tr.Loss: 0.416956, Val.Loss: 0.419091, Tr.Acc: 0.816, Val.Acc: 0.819\n",
      "Epoch: 111, Tr.Loss: 0.414822, Val.Loss: 0.417800, Tr.Acc: 0.816, Val.Acc: 0.818\n",
      "Epoch: 112, Tr.Loss: 0.413617, Val.Loss: 0.416582, Tr.Acc: 0.817, Val.Acc: 0.818\n",
      "Epoch: 113, Tr.Loss: 0.411893, Val.Loss: 0.415251, Tr.Acc: 0.818, Val.Acc: 0.817\n",
      "Epoch: 114, Tr.Loss: 0.410670, Val.Loss: 0.414184, Tr.Acc: 0.818, Val.Acc: 0.818\n",
      "Epoch: 115, Tr.Loss: 0.408555, Val.Loss: 0.412963, Tr.Acc: 0.819, Val.Acc: 0.817\n",
      "Epoch: 116, Tr.Loss: 0.407299, Val.Loss: 0.411793, Tr.Acc: 0.820, Val.Acc: 0.819\n",
      "Epoch: 117, Tr.Loss: 0.405484, Val.Loss: 0.410743, Tr.Acc: 0.820, Val.Acc: 0.818\n",
      "Epoch: 118, Tr.Loss: 0.403748, Val.Loss: 0.409563, Tr.Acc: 0.821, Val.Acc: 0.820\n",
      "Epoch: 119, Tr.Loss: 0.402182, Val.Loss: 0.408282, Tr.Acc: 0.823, Val.Acc: 0.821\n",
      "Epoch: 120, Tr.Loss: 0.401595, Val.Loss: 0.407255, Tr.Acc: 0.823, Val.Acc: 0.821\n",
      "Epoch: 121, Tr.Loss: 0.399280, Val.Loss: 0.406140, Tr.Acc: 0.823, Val.Acc: 0.822\n",
      "Epoch: 122, Tr.Loss: 0.398249, Val.Loss: 0.405145, Tr.Acc: 0.824, Val.Acc: 0.825\n",
      "Epoch: 123, Tr.Loss: 0.396241, Val.Loss: 0.403982, Tr.Acc: 0.826, Val.Acc: 0.824\n",
      "Epoch: 124, Tr.Loss: 0.394395, Val.Loss: 0.402977, Tr.Acc: 0.827, Val.Acc: 0.825\n",
      "Epoch: 125, Tr.Loss: 0.393932, Val.Loss: 0.402139, Tr.Acc: 0.828, Val.Acc: 0.826\n",
      "Epoch: 126, Tr.Loss: 0.392944, Val.Loss: 0.401101, Tr.Acc: 0.829, Val.Acc: 0.827\n",
      "Epoch: 127, Tr.Loss: 0.390918, Val.Loss: 0.400153, Tr.Acc: 0.830, Val.Acc: 0.828\n",
      "Epoch: 128, Tr.Loss: 0.389167, Val.Loss: 0.399135, Tr.Acc: 0.831, Val.Acc: 0.830\n",
      "Epoch: 129, Tr.Loss: 0.388191, Val.Loss: 0.398141, Tr.Acc: 0.831, Val.Acc: 0.829\n",
      "Epoch: 130, Tr.Loss: 0.386593, Val.Loss: 0.397165, Tr.Acc: 0.831, Val.Acc: 0.830\n",
      "Epoch: 131, Tr.Loss: 0.385065, Val.Loss: 0.396190, Tr.Acc: 0.832, Val.Acc: 0.831\n",
      "Epoch: 132, Tr.Loss: 0.383279, Val.Loss: 0.395222, Tr.Acc: 0.833, Val.Acc: 0.830\n",
      "Epoch: 133, Tr.Loss: 0.382716, Val.Loss: 0.394340, Tr.Acc: 0.833, Val.Acc: 0.830\n",
      "Epoch: 134, Tr.Loss: 0.380580, Val.Loss: 0.393346, Tr.Acc: 0.835, Val.Acc: 0.830\n",
      "Epoch: 135, Tr.Loss: 0.379876, Val.Loss: 0.392418, Tr.Acc: 0.836, Val.Acc: 0.831\n",
      "Epoch: 136, Tr.Loss: 0.378105, Val.Loss: 0.391543, Tr.Acc: 0.837, Val.Acc: 0.833\n",
      "Epoch: 137, Tr.Loss: 0.376471, Val.Loss: 0.390581, Tr.Acc: 0.838, Val.Acc: 0.832\n",
      "Epoch: 138, Tr.Loss: 0.375550, Val.Loss: 0.389895, Tr.Acc: 0.838, Val.Acc: 0.832\n",
      "Epoch: 139, Tr.Loss: 0.374091, Val.Loss: 0.388877, Tr.Acc: 0.839, Val.Acc: 0.832\n",
      "Epoch: 140, Tr.Loss: 0.372593, Val.Loss: 0.388340, Tr.Acc: 0.839, Val.Acc: 0.831\n",
      "Epoch: 141, Tr.Loss: 0.371026, Val.Loss: 0.387315, Tr.Acc: 0.840, Val.Acc: 0.832\n",
      "Epoch: 142, Tr.Loss: 0.369692, Val.Loss: 0.386572, Tr.Acc: 0.840, Val.Acc: 0.833\n",
      "Epoch: 143, Tr.Loss: 0.368553, Val.Loss: 0.385720, Tr.Acc: 0.841, Val.Acc: 0.833\n",
      "Epoch: 144, Tr.Loss: 0.366899, Val.Loss: 0.384806, Tr.Acc: 0.842, Val.Acc: 0.834\n",
      "Epoch: 145, Tr.Loss: 0.366085, Val.Loss: 0.383995, Tr.Acc: 0.843, Val.Acc: 0.833\n",
      "Epoch: 146, Tr.Loss: 0.364953, Val.Loss: 0.383167, Tr.Acc: 0.844, Val.Acc: 0.834\n",
      "Epoch: 147, Tr.Loss: 0.363987, Val.Loss: 0.382355, Tr.Acc: 0.845, Val.Acc: 0.835\n",
      "Epoch: 148, Tr.Loss: 0.362220, Val.Loss: 0.381875, Tr.Acc: 0.846, Val.Acc: 0.834\n",
      "Epoch: 149, Tr.Loss: 0.360757, Val.Loss: 0.381106, Tr.Acc: 0.847, Val.Acc: 0.833\n",
      "Epoch: 150, Tr.Loss: 0.359976, Val.Loss: 0.380072, Tr.Acc: 0.847, Val.Acc: 0.833\n",
      "Epoch: 151, Tr.Loss: 0.358375, Val.Loss: 0.379217, Tr.Acc: 0.848, Val.Acc: 0.833\n",
      "Epoch: 152, Tr.Loss: 0.357213, Val.Loss: 0.378566, Tr.Acc: 0.849, Val.Acc: 0.833\n",
      "Epoch: 153, Tr.Loss: 0.356220, Val.Loss: 0.377931, Tr.Acc: 0.849, Val.Acc: 0.832\n",
      "Epoch: 154, Tr.Loss: 0.355124, Val.Loss: 0.377077, Tr.Acc: 0.850, Val.Acc: 0.833\n",
      "Epoch: 155, Tr.Loss: 0.352921, Val.Loss: 0.376518, Tr.Acc: 0.851, Val.Acc: 0.833\n",
      "Epoch: 156, Tr.Loss: 0.351610, Val.Loss: 0.376314, Tr.Acc: 0.851, Val.Acc: 0.832\n",
      "Epoch: 157, Tr.Loss: 0.350590, Val.Loss: 0.375382, Tr.Acc: 0.852, Val.Acc: 0.833\n",
      "Epoch: 158, Tr.Loss: 0.348882, Val.Loss: 0.374547, Tr.Acc: 0.853, Val.Acc: 0.834\n",
      "Epoch: 159, Tr.Loss: 0.348176, Val.Loss: 0.373911, Tr.Acc: 0.853, Val.Acc: 0.834\n",
      "Epoch: 160, Tr.Loss: 0.347144, Val.Loss: 0.373141, Tr.Acc: 0.853, Val.Acc: 0.835\n",
      "Epoch: 161, Tr.Loss: 0.345889, Val.Loss: 0.372479, Tr.Acc: 0.854, Val.Acc: 0.834\n",
      "Epoch: 162, Tr.Loss: 0.344646, Val.Loss: 0.372030, Tr.Acc: 0.854, Val.Acc: 0.833\n",
      "Epoch: 163, Tr.Loss: 0.343534, Val.Loss: 0.371427, Tr.Acc: 0.854, Val.Acc: 0.833\n",
      "Epoch: 164, Tr.Loss: 0.342442, Val.Loss: 0.370955, Tr.Acc: 0.854, Val.Acc: 0.832\n",
      "Epoch: 165, Tr.Loss: 0.340801, Val.Loss: 0.370387, Tr.Acc: 0.856, Val.Acc: 0.833\n",
      "Epoch: 166, Tr.Loss: 0.339573, Val.Loss: 0.369608, Tr.Acc: 0.856, Val.Acc: 0.834\n",
      "Epoch: 167, Tr.Loss: 0.338284, Val.Loss: 0.369061, Tr.Acc: 0.857, Val.Acc: 0.834\n",
      "Epoch: 168, Tr.Loss: 0.336867, Val.Loss: 0.368519, Tr.Acc: 0.857, Val.Acc: 0.836\n",
      "Epoch: 169, Tr.Loss: 0.335895, Val.Loss: 0.368009, Tr.Acc: 0.858, Val.Acc: 0.836\n",
      "Epoch: 170, Tr.Loss: 0.334971, Val.Loss: 0.367644, Tr.Acc: 0.858, Val.Acc: 0.837\n",
      "Epoch: 171, Tr.Loss: 0.333678, Val.Loss: 0.366874, Tr.Acc: 0.859, Val.Acc: 0.837\n",
      "Epoch: 172, Tr.Loss: 0.332605, Val.Loss: 0.366598, Tr.Acc: 0.858, Val.Acc: 0.837\n",
      "Epoch: 173, Tr.Loss: 0.331221, Val.Loss: 0.365574, Tr.Acc: 0.860, Val.Acc: 0.838\n",
      "Epoch: 174, Tr.Loss: 0.329976, Val.Loss: 0.365255, Tr.Acc: 0.861, Val.Acc: 0.837\n",
      "Epoch: 175, Tr.Loss: 0.328492, Val.Loss: 0.364737, Tr.Acc: 0.862, Val.Acc: 0.838\n",
      "\n",
      "Testing the model .... \n",
      "\n",
      "Test Loss: 0.389340, Test Accuracy: 0.823\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filtering the data set to include only the two classes\n",
    "classes = [b'airplane', b'ship']\n",
    "class0 = labelNames.index(classes[0])\n",
    "class1 = labelNames.index(classes[1])\n",
    "\n",
    "trainX = trainImages[(trainLabels == class0) | (trainLabels == class1)]\n",
    "trainY = trainLabels[(trainLabels == class0) | (trainLabels == class1)]\n",
    "trainY[trainY == class0] = 0\n",
    "trainY[trainY == class1] = 1\n",
    "\n",
    "testX = testImages[(testLabels == class0) | (testLabels == class1)]\n",
    "testY = testLabels[(testLabels == class0) | (testLabels == class1)]\n",
    "testY[testY == class0] = 0\n",
    "testY[testY == class1] = 1\n",
    "\n",
    "\n",
    "#extracting the HOG features\n",
    "trainX = np.array([hog(image, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                       transform_sqrt=True, block_norm=\"L1\") for image in trainX])\n",
    "testX = np.array([hog(image, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                       transform_sqrt=True, block_norm=\"L1\") for image in testX])\n",
    "\n",
    "\n",
    "#normalizing the data\n",
    "mean = np.mean(trainX, axis=0, keepdims=True)\n",
    "std = np.std(trainX, axis=0, keepdims=True)\n",
    "trainX = (trainX - mean)/std\n",
    "testX = (testX - mean)/std\n",
    "\n",
    "\n",
    "#values of hyperparameters\n",
    "SEED = 100\n",
    "VAL_SPLIT = 0.2\n",
    "BATCH_SIZE = 1024\n",
    "NUM_EPOCHS = 175\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "#splitting the training set into training and validation sets\n",
    "np.random.seed(SEED)\n",
    "indices = np.arange(len(trainX))\n",
    "np.random.shuffle(indices)\n",
    "valX = trainX[indices[:int(VAL_SPLIT*len(indices))]]\n",
    "valY = trainY[indices[:int(VAL_SPLIT*len(indices))]]\n",
    "trainX = trainX[indices[int(VAL_SPLIT*len(indices)):]]\n",
    "trainY = trainY[indices[int(VAL_SPLIT*len(indices)):]]\n",
    "\n",
    "\n",
    "#Model architecture\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = Linear(288,512,LEARNING_RATE)\n",
    "        self.afunc1 = ReLU()\n",
    "        self.fc2 = Linear(512,128,LEARNING_RATE)\n",
    "        self.afunc2 = ReLU()\n",
    "        self.fc3 = Linear(128,32,LEARNING_RATE)\n",
    "        self.afunc3 = ReLU()\n",
    "        self.fc4 = Linear(32,2,LEARNING_RATE)\n",
    "        self.afunc4 = Softmax()\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.afunc1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.afunc2.forward(x)\n",
    "        x = self.fc3.forward(x)\n",
    "        x = self.afunc3.forward(x)\n",
    "        x = self.fc4.forward(x)\n",
    "        x = self.afunc4.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        gradients = self.afunc4.backward(gradients)\n",
    "        gradients = self.fc4.backward(gradients)\n",
    "        gradients = self.afunc3.backward(gradients)\n",
    "        gradients = self.fc3.backward(gradients)\n",
    "        gradients = self.afunc2.backward(gradients)\n",
    "        gradients = self.fc2.backward(gradients)\n",
    "        gradients = self.afunc1.backward(gradients)\n",
    "        gradients = self.fc1.backward(gradients)\n",
    "        return\n",
    "    \n",
    "    def step(self):\n",
    "        self.fc1.step()\n",
    "        self.fc2.step()\n",
    "        self.fc3.step()\n",
    "        self.fc4.step()\n",
    "        return\n",
    "    \n",
    "    def num_params(self):\n",
    "        numParams = 0\n",
    "        numParams = numParams + self.fc1.num_params()\n",
    "        numParams = numParams + self.fc2.num_params()\n",
    "        numParams = numParams + self.fc3.num_params()\n",
    "        numParams = numParams + self.fc4.num_params()\n",
    "        return numParams\n",
    "    \n",
    "    \n",
    "#loss function and model initialization\n",
    "loss_function = CrossEntropyLoss()\n",
    "model = Net()\n",
    "print(\"\\nNumber of Parameters in Model = \" + str(model.num_params()))\n",
    "\n",
    "\n",
    "print('\\nTraining the model .... \\n')\n",
    "trainingLossCurve = []\n",
    "validationLossCurve = []\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    \n",
    "    #shuffle the dataset so the composition of batches differs every epoch\n",
    "    indices = np.arange(len(trainX))\n",
    "    np.random.shuffle(indices)\n",
    "    trainX = trainX[indices]\n",
    "    trainY = trainY[indices]\n",
    "    \n",
    "    #train for one epoch and obtain the training set loss and accuracy\n",
    "    trainingLoss, trainingAccuracy = train(model, trainX, trainY, loss_function)\n",
    "    trainingLossCurve.append(trainingLoss)\n",
    "    \n",
    "    #obtain the validation set loss and accuracy\n",
    "    validationLoss, validationAccuracy = validate(model, valX, valY, loss_function)\n",
    "    validationLossCurve.append(validationLoss)\n",
    "    print('Epoch: %d, Tr.Loss: %.6f, Val.Loss: %.6f, Tr.Acc: %.3f, Val.Acc: %.3f' \n",
    "          %(epoch, trainingLoss, validationLoss, trainingAccuracy, validationAccuracy))\n",
    "\n",
    "\n",
    "    \n",
    "print('\\nTesting the model .... \\n')\n",
    "#obtain the test set loss and accuracy\n",
    "testLoss, testAccuracy = test(model, testX, testY, loss_function)\n",
    "print('Test Loss: %.6f, Test Accuracy: %.3f' %(testLoss, testAccuracy))\n",
    "\n",
    "\n",
    "#plot the training loss and validation loss curves\n",
    "plt.figure()\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epoch No.')\n",
    "plt.ylabel('Loss value')\n",
    "plt.plot(trainingLossCurve, 'b', label='Train')\n",
    "plt.plot(validationLossCurve, 'r', label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Five classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter the data set to include the `Cat`, `Deer`, `Dog`, `Frog` and `Horse` classes. We assign labels from $0$ to $4$ in the given order to the classes. We use the Histogram of Oriented Gradients (HOG) Descriptors as feature vectors. We extract $288$ HOG features for each image. We normalize the data using `mean` and `std` and split the training set into training ($80\\%$) and validation ($20\\%$) sets. \n",
    "\n",
    "\n",
    "The Neural Network architecture used consists of $1$ input layer, $4$ hidden layers each with `ReLU` activation and $1$ output layer with `Softmax` activation so that the model output is a probability distribution. We use the `Cross Entropy` Loss function to train the network.\n",
    "\n",
    "The hyperparameter values used are:\n",
    "- Batch Size = $512$\n",
    "- Epochs = $250$\n",
    "- Learning Rate = $0.005$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of Parameters in Model = 296869\n",
      "\n",
      "Training the model .... \n",
      "\n",
      "Epoch: 1, Tr.Loss: 1.610659, Val.Loss: 1.611731, Tr.Acc: 0.200, Val.Acc: 0.191\n",
      "Epoch: 2, Tr.Loss: 1.610388, Val.Loss: 1.611154, Tr.Acc: 0.199, Val.Acc: 0.191\n",
      "Epoch: 3, Tr.Loss: 1.609369, Val.Loss: 1.610614, Tr.Acc: 0.201, Val.Acc: 0.191\n",
      "Epoch: 4, Tr.Loss: 1.609076, Val.Loss: 1.610086, Tr.Acc: 0.200, Val.Acc: 0.191\n",
      "Epoch: 5, Tr.Loss: 1.608740, Val.Loss: 1.609565, Tr.Acc: 0.199, Val.Acc: 0.191\n",
      "Epoch: 6, Tr.Loss: 1.607420, Val.Loss: 1.609146, Tr.Acc: 0.206, Val.Acc: 0.191\n",
      "Epoch: 7, Tr.Loss: 1.607279, Val.Loss: 1.608735, Tr.Acc: 0.204, Val.Acc: 0.191\n",
      "Epoch: 8, Tr.Loss: 1.607429, Val.Loss: 1.608278, Tr.Acc: 0.201, Val.Acc: 0.191\n",
      "Epoch: 9, Tr.Loss: 1.607052, Val.Loss: 1.607827, Tr.Acc: 0.200, Val.Acc: 0.192\n",
      "Epoch: 10, Tr.Loss: 1.606455, Val.Loss: 1.607358, Tr.Acc: 0.201, Val.Acc: 0.193\n",
      "Epoch: 11, Tr.Loss: 1.606151, Val.Loss: 1.606965, Tr.Acc: 0.204, Val.Acc: 0.195\n",
      "Epoch: 12, Tr.Loss: 1.605361, Val.Loss: 1.606582, Tr.Acc: 0.211, Val.Acc: 0.197\n",
      "Epoch: 13, Tr.Loss: 1.604882, Val.Loss: 1.606205, Tr.Acc: 0.214, Val.Acc: 0.199\n",
      "Epoch: 14, Tr.Loss: 1.604871, Val.Loss: 1.605777, Tr.Acc: 0.214, Val.Acc: 0.201\n",
      "Epoch: 15, Tr.Loss: 1.604163, Val.Loss: 1.605370, Tr.Acc: 0.220, Val.Acc: 0.204\n",
      "Epoch: 16, Tr.Loss: 1.603975, Val.Loss: 1.604912, Tr.Acc: 0.225, Val.Acc: 0.209\n",
      "Epoch: 17, Tr.Loss: 1.603674, Val.Loss: 1.604473, Tr.Acc: 0.232, Val.Acc: 0.215\n",
      "Epoch: 18, Tr.Loss: 1.602790, Val.Loss: 1.604073, Tr.Acc: 0.239, Val.Acc: 0.219\n",
      "Epoch: 19, Tr.Loss: 1.602945, Val.Loss: 1.603575, Tr.Acc: 0.239, Val.Acc: 0.226\n",
      "Epoch: 20, Tr.Loss: 1.602279, Val.Loss: 1.603087, Tr.Acc: 0.246, Val.Acc: 0.233\n",
      "Epoch: 21, Tr.Loss: 1.601652, Val.Loss: 1.602556, Tr.Acc: 0.256, Val.Acc: 0.243\n",
      "Epoch: 22, Tr.Loss: 1.601181, Val.Loss: 1.602006, Tr.Acc: 0.266, Val.Acc: 0.250\n",
      "Epoch: 23, Tr.Loss: 1.600561, Val.Loss: 1.601447, Tr.Acc: 0.273, Val.Acc: 0.255\n",
      "Epoch: 24, Tr.Loss: 1.599894, Val.Loss: 1.600839, Tr.Acc: 0.278, Val.Acc: 0.264\n",
      "Epoch: 25, Tr.Loss: 1.599348, Val.Loss: 1.600216, Tr.Acc: 0.280, Val.Acc: 0.267\n",
      "Epoch: 26, Tr.Loss: 1.598808, Val.Loss: 1.599556, Tr.Acc: 0.286, Val.Acc: 0.274\n",
      "Epoch: 27, Tr.Loss: 1.597865, Val.Loss: 1.598884, Tr.Acc: 0.292, Val.Acc: 0.279\n",
      "Epoch: 28, Tr.Loss: 1.597422, Val.Loss: 1.598181, Tr.Acc: 0.294, Val.Acc: 0.282\n",
      "Epoch: 29, Tr.Loss: 1.596732, Val.Loss: 1.597402, Tr.Acc: 0.296, Val.Acc: 0.285\n",
      "Epoch: 30, Tr.Loss: 1.595489, Val.Loss: 1.596553, Tr.Acc: 0.300, Val.Acc: 0.287\n",
      "Epoch: 31, Tr.Loss: 1.594868, Val.Loss: 1.595695, Tr.Acc: 0.304, Val.Acc: 0.289\n",
      "Epoch: 32, Tr.Loss: 1.593644, Val.Loss: 1.594747, Tr.Acc: 0.307, Val.Acc: 0.294\n",
      "Epoch: 33, Tr.Loss: 1.592863, Val.Loss: 1.593779, Tr.Acc: 0.307, Val.Acc: 0.294\n",
      "Epoch: 34, Tr.Loss: 1.592075, Val.Loss: 1.592758, Tr.Acc: 0.307, Val.Acc: 0.296\n",
      "Epoch: 35, Tr.Loss: 1.590762, Val.Loss: 1.591685, Tr.Acc: 0.311, Val.Acc: 0.297\n",
      "Epoch: 36, Tr.Loss: 1.589636, Val.Loss: 1.590543, Tr.Acc: 0.313, Val.Acc: 0.299\n",
      "Epoch: 37, Tr.Loss: 1.588628, Val.Loss: 1.589336, Tr.Acc: 0.315, Val.Acc: 0.301\n",
      "Epoch: 38, Tr.Loss: 1.587434, Val.Loss: 1.588143, Tr.Acc: 0.316, Val.Acc: 0.303\n",
      "Epoch: 39, Tr.Loss: 1.585930, Val.Loss: 1.586797, Tr.Acc: 0.320, Val.Acc: 0.305\n",
      "Epoch: 40, Tr.Loss: 1.584359, Val.Loss: 1.585406, Tr.Acc: 0.322, Val.Acc: 0.309\n",
      "Epoch: 41, Tr.Loss: 1.582471, Val.Loss: 1.583938, Tr.Acc: 0.323, Val.Acc: 0.310\n",
      "Epoch: 42, Tr.Loss: 1.581755, Val.Loss: 1.582393, Tr.Acc: 0.323, Val.Acc: 0.314\n",
      "Epoch: 43, Tr.Loss: 1.579781, Val.Loss: 1.580700, Tr.Acc: 0.325, Val.Acc: 0.315\n",
      "Epoch: 44, Tr.Loss: 1.577429, Val.Loss: 1.578887, Tr.Acc: 0.328, Val.Acc: 0.315\n",
      "Epoch: 45, Tr.Loss: 1.575847, Val.Loss: 1.576953, Tr.Acc: 0.332, Val.Acc: 0.318\n",
      "Epoch: 46, Tr.Loss: 1.573681, Val.Loss: 1.574946, Tr.Acc: 0.334, Val.Acc: 0.320\n",
      "Epoch: 47, Tr.Loss: 1.571291, Val.Loss: 1.572794, Tr.Acc: 0.339, Val.Acc: 0.324\n",
      "Epoch: 48, Tr.Loss: 1.569427, Val.Loss: 1.570630, Tr.Acc: 0.343, Val.Acc: 0.327\n",
      "Epoch: 49, Tr.Loss: 1.566414, Val.Loss: 1.568290, Tr.Acc: 0.345, Val.Acc: 0.329\n",
      "Epoch: 50, Tr.Loss: 1.564214, Val.Loss: 1.565760, Tr.Acc: 0.347, Val.Acc: 0.333\n",
      "Epoch: 51, Tr.Loss: 1.561345, Val.Loss: 1.563152, Tr.Acc: 0.349, Val.Acc: 0.335\n",
      "Epoch: 52, Tr.Loss: 1.559467, Val.Loss: 1.560539, Tr.Acc: 0.352, Val.Acc: 0.337\n",
      "Epoch: 53, Tr.Loss: 1.555649, Val.Loss: 1.557678, Tr.Acc: 0.358, Val.Acc: 0.340\n",
      "Epoch: 54, Tr.Loss: 1.551668, Val.Loss: 1.554642, Tr.Acc: 0.363, Val.Acc: 0.343\n",
      "Epoch: 55, Tr.Loss: 1.549448, Val.Loss: 1.551458, Tr.Acc: 0.358, Val.Acc: 0.343\n",
      "Epoch: 56, Tr.Loss: 1.546135, Val.Loss: 1.548281, Tr.Acc: 0.364, Val.Acc: 0.346\n",
      "Epoch: 57, Tr.Loss: 1.543339, Val.Loss: 1.545043, Tr.Acc: 0.363, Val.Acc: 0.349\n",
      "Epoch: 58, Tr.Loss: 1.539358, Val.Loss: 1.541648, Tr.Acc: 0.369, Val.Acc: 0.351\n",
      "Epoch: 59, Tr.Loss: 1.536285, Val.Loss: 1.538153, Tr.Acc: 0.369, Val.Acc: 0.353\n",
      "Epoch: 60, Tr.Loss: 1.531493, Val.Loss: 1.534514, Tr.Acc: 0.368, Val.Acc: 0.356\n",
      "Epoch: 61, Tr.Loss: 1.527104, Val.Loss: 1.530827, Tr.Acc: 0.373, Val.Acc: 0.358\n",
      "Epoch: 62, Tr.Loss: 1.523843, Val.Loss: 1.527234, Tr.Acc: 0.371, Val.Acc: 0.361\n",
      "Epoch: 63, Tr.Loss: 1.521147, Val.Loss: 1.523539, Tr.Acc: 0.372, Val.Acc: 0.362\n",
      "Epoch: 64, Tr.Loss: 1.515754, Val.Loss: 1.519682, Tr.Acc: 0.375, Val.Acc: 0.363\n",
      "Epoch: 65, Tr.Loss: 1.511860, Val.Loss: 1.515795, Tr.Acc: 0.380, Val.Acc: 0.365\n",
      "Epoch: 66, Tr.Loss: 1.508954, Val.Loss: 1.511779, Tr.Acc: 0.382, Val.Acc: 0.371\n",
      "Epoch: 67, Tr.Loss: 1.504486, Val.Loss: 1.507728, Tr.Acc: 0.385, Val.Acc: 0.374\n",
      "Epoch: 68, Tr.Loss: 1.499387, Val.Loss: 1.503655, Tr.Acc: 0.388, Val.Acc: 0.375\n",
      "Epoch: 69, Tr.Loss: 1.494609, Val.Loss: 1.499469, Tr.Acc: 0.386, Val.Acc: 0.379\n",
      "Epoch: 70, Tr.Loss: 1.489253, Val.Loss: 1.495066, Tr.Acc: 0.393, Val.Acc: 0.382\n",
      "Epoch: 71, Tr.Loss: 1.482982, Val.Loss: 1.490506, Tr.Acc: 0.399, Val.Acc: 0.382\n",
      "Epoch: 72, Tr.Loss: 1.478748, Val.Loss: 1.485781, Tr.Acc: 0.399, Val.Acc: 0.384\n",
      "Epoch: 73, Tr.Loss: 1.474002, Val.Loss: 1.481002, Tr.Acc: 0.401, Val.Acc: 0.386\n",
      "Epoch: 74, Tr.Loss: 1.469919, Val.Loss: 1.476077, Tr.Acc: 0.400, Val.Acc: 0.387\n",
      "Epoch: 75, Tr.Loss: 1.464623, Val.Loss: 1.470916, Tr.Acc: 0.403, Val.Acc: 0.393\n",
      "Epoch: 76, Tr.Loss: 1.458315, Val.Loss: 1.465475, Tr.Acc: 0.405, Val.Acc: 0.394\n",
      "Epoch: 77, Tr.Loss: 1.451772, Val.Loss: 1.459874, Tr.Acc: 0.407, Val.Acc: 0.397\n",
      "Epoch: 78, Tr.Loss: 1.444235, Val.Loss: 1.453972, Tr.Acc: 0.412, Val.Acc: 0.399\n",
      "Epoch: 79, Tr.Loss: 1.438468, Val.Loss: 1.447588, Tr.Acc: 0.413, Val.Acc: 0.402\n",
      "Epoch: 80, Tr.Loss: 1.431749, Val.Loss: 1.441231, Tr.Acc: 0.414, Val.Acc: 0.404\n",
      "Epoch: 81, Tr.Loss: 1.423831, Val.Loss: 1.434432, Tr.Acc: 0.415, Val.Acc: 0.406\n",
      "Epoch: 82, Tr.Loss: 1.415558, Val.Loss: 1.427322, Tr.Acc: 0.422, Val.Acc: 0.407\n",
      "Epoch: 83, Tr.Loss: 1.411641, Val.Loss: 1.420463, Tr.Acc: 0.420, Val.Acc: 0.411\n",
      "Epoch: 84, Tr.Loss: 1.402628, Val.Loss: 1.413047, Tr.Acc: 0.423, Val.Acc: 0.414\n",
      "Epoch: 85, Tr.Loss: 1.393758, Val.Loss: 1.405732, Tr.Acc: 0.431, Val.Acc: 0.419\n",
      "Epoch: 86, Tr.Loss: 1.386131, Val.Loss: 1.398321, Tr.Acc: 0.430, Val.Acc: 0.421\n",
      "Epoch: 87, Tr.Loss: 1.378251, Val.Loss: 1.390772, Tr.Acc: 0.435, Val.Acc: 0.424\n",
      "Epoch: 88, Tr.Loss: 1.370568, Val.Loss: 1.383494, Tr.Acc: 0.437, Val.Acc: 0.428\n",
      "Epoch: 89, Tr.Loss: 1.360366, Val.Loss: 1.376015, Tr.Acc: 0.440, Val.Acc: 0.432\n",
      "Epoch: 90, Tr.Loss: 1.353137, Val.Loss: 1.368608, Tr.Acc: 0.443, Val.Acc: 0.435\n",
      "Epoch: 91, Tr.Loss: 1.345367, Val.Loss: 1.361544, Tr.Acc: 0.444, Val.Acc: 0.438\n",
      "Epoch: 92, Tr.Loss: 1.333813, Val.Loss: 1.354712, Tr.Acc: 0.453, Val.Acc: 0.441\n",
      "Epoch: 93, Tr.Loss: 1.330150, Val.Loss: 1.348038, Tr.Acc: 0.452, Val.Acc: 0.440\n",
      "Epoch: 94, Tr.Loss: 1.322578, Val.Loss: 1.341824, Tr.Acc: 0.457, Val.Acc: 0.445\n",
      "Epoch: 95, Tr.Loss: 1.316013, Val.Loss: 1.335805, Tr.Acc: 0.457, Val.Acc: 0.447\n",
      "Epoch: 96, Tr.Loss: 1.312175, Val.Loss: 1.330066, Tr.Acc: 0.463, Val.Acc: 0.446\n",
      "Epoch: 97, Tr.Loss: 1.303979, Val.Loss: 1.324187, Tr.Acc: 0.465, Val.Acc: 0.451\n",
      "Epoch: 98, Tr.Loss: 1.295092, Val.Loss: 1.318630, Tr.Acc: 0.472, Val.Acc: 0.454\n",
      "Epoch: 99, Tr.Loss: 1.289465, Val.Loss: 1.313685, Tr.Acc: 0.472, Val.Acc: 0.454\n",
      "Epoch: 100, Tr.Loss: 1.283843, Val.Loss: 1.308593, Tr.Acc: 0.472, Val.Acc: 0.454\n",
      "Epoch: 101, Tr.Loss: 1.275860, Val.Loss: 1.303884, Tr.Acc: 0.474, Val.Acc: 0.455\n",
      "Epoch: 102, Tr.Loss: 1.276703, Val.Loss: 1.299305, Tr.Acc: 0.478, Val.Acc: 0.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103, Tr.Loss: 1.270496, Val.Loss: 1.295080, Tr.Acc: 0.481, Val.Acc: 0.464\n",
      "Epoch: 104, Tr.Loss: 1.265754, Val.Loss: 1.291312, Tr.Acc: 0.486, Val.Acc: 0.466\n",
      "Epoch: 105, Tr.Loss: 1.258462, Val.Loss: 1.287357, Tr.Acc: 0.486, Val.Acc: 0.467\n",
      "Epoch: 106, Tr.Loss: 1.252165, Val.Loss: 1.283303, Tr.Acc: 0.490, Val.Acc: 0.469\n",
      "Epoch: 107, Tr.Loss: 1.255240, Val.Loss: 1.279760, Tr.Acc: 0.487, Val.Acc: 0.471\n",
      "Epoch: 108, Tr.Loss: 1.247500, Val.Loss: 1.276314, Tr.Acc: 0.490, Val.Acc: 0.472\n",
      "Epoch: 109, Tr.Loss: 1.241020, Val.Loss: 1.273659, Tr.Acc: 0.495, Val.Acc: 0.472\n",
      "Epoch: 110, Tr.Loss: 1.236813, Val.Loss: 1.269201, Tr.Acc: 0.497, Val.Acc: 0.478\n",
      "Epoch: 111, Tr.Loss: 1.233340, Val.Loss: 1.266140, Tr.Acc: 0.497, Val.Acc: 0.477\n",
      "Epoch: 112, Tr.Loss: 1.228906, Val.Loss: 1.262939, Tr.Acc: 0.499, Val.Acc: 0.481\n",
      "Epoch: 113, Tr.Loss: 1.229660, Val.Loss: 1.260437, Tr.Acc: 0.501, Val.Acc: 0.480\n",
      "Epoch: 114, Tr.Loss: 1.225504, Val.Loss: 1.256882, Tr.Acc: 0.504, Val.Acc: 0.482\n",
      "Epoch: 115, Tr.Loss: 1.220699, Val.Loss: 1.254336, Tr.Acc: 0.506, Val.Acc: 0.484\n",
      "Epoch: 116, Tr.Loss: 1.215645, Val.Loss: 1.251640, Tr.Acc: 0.508, Val.Acc: 0.484\n",
      "Epoch: 117, Tr.Loss: 1.212570, Val.Loss: 1.248743, Tr.Acc: 0.509, Val.Acc: 0.487\n",
      "Epoch: 118, Tr.Loss: 1.203864, Val.Loss: 1.246237, Tr.Acc: 0.514, Val.Acc: 0.487\n",
      "Epoch: 119, Tr.Loss: 1.212120, Val.Loss: 1.242988, Tr.Acc: 0.509, Val.Acc: 0.489\n",
      "Epoch: 120, Tr.Loss: 1.203079, Val.Loss: 1.240229, Tr.Acc: 0.512, Val.Acc: 0.491\n",
      "Epoch: 121, Tr.Loss: 1.199341, Val.Loss: 1.237289, Tr.Acc: 0.513, Val.Acc: 0.494\n",
      "Epoch: 122, Tr.Loss: 1.193399, Val.Loss: 1.234163, Tr.Acc: 0.519, Val.Acc: 0.495\n",
      "Epoch: 123, Tr.Loss: 1.191235, Val.Loss: 1.231310, Tr.Acc: 0.518, Val.Acc: 0.499\n",
      "Epoch: 124, Tr.Loss: 1.191251, Val.Loss: 1.229285, Tr.Acc: 0.522, Val.Acc: 0.497\n",
      "Epoch: 125, Tr.Loss: 1.186058, Val.Loss: 1.225588, Tr.Acc: 0.522, Val.Acc: 0.503\n",
      "Epoch: 126, Tr.Loss: 1.180807, Val.Loss: 1.222951, Tr.Acc: 0.528, Val.Acc: 0.503\n",
      "Epoch: 127, Tr.Loss: 1.179365, Val.Loss: 1.219948, Tr.Acc: 0.530, Val.Acc: 0.505\n",
      "Epoch: 128, Tr.Loss: 1.171210, Val.Loss: 1.217233, Tr.Acc: 0.532, Val.Acc: 0.506\n",
      "Epoch: 129, Tr.Loss: 1.170538, Val.Loss: 1.214688, Tr.Acc: 0.533, Val.Acc: 0.506\n",
      "Epoch: 130, Tr.Loss: 1.172765, Val.Loss: 1.212109, Tr.Acc: 0.531, Val.Acc: 0.506\n",
      "Epoch: 131, Tr.Loss: 1.164524, Val.Loss: 1.208705, Tr.Acc: 0.538, Val.Acc: 0.510\n",
      "Epoch: 132, Tr.Loss: 1.158703, Val.Loss: 1.206249, Tr.Acc: 0.538, Val.Acc: 0.511\n",
      "Epoch: 133, Tr.Loss: 1.155009, Val.Loss: 1.203591, Tr.Acc: 0.543, Val.Acc: 0.511\n",
      "Epoch: 134, Tr.Loss: 1.148706, Val.Loss: 1.200272, Tr.Acc: 0.543, Val.Acc: 0.514\n",
      "Epoch: 135, Tr.Loss: 1.148483, Val.Loss: 1.197790, Tr.Acc: 0.545, Val.Acc: 0.513\n",
      "Epoch: 136, Tr.Loss: 1.140465, Val.Loss: 1.194252, Tr.Acc: 0.551, Val.Acc: 0.516\n",
      "Epoch: 137, Tr.Loss: 1.138525, Val.Loss: 1.191894, Tr.Acc: 0.549, Val.Acc: 0.516\n",
      "Epoch: 138, Tr.Loss: 1.141097, Val.Loss: 1.190622, Tr.Acc: 0.547, Val.Acc: 0.516\n",
      "Epoch: 139, Tr.Loss: 1.133317, Val.Loss: 1.186261, Tr.Acc: 0.554, Val.Acc: 0.516\n",
      "Epoch: 140, Tr.Loss: 1.126473, Val.Loss: 1.182700, Tr.Acc: 0.555, Val.Acc: 0.521\n",
      "Epoch: 141, Tr.Loss: 1.124784, Val.Loss: 1.180006, Tr.Acc: 0.557, Val.Acc: 0.521\n",
      "Epoch: 142, Tr.Loss: 1.119011, Val.Loss: 1.177282, Tr.Acc: 0.560, Val.Acc: 0.523\n",
      "Epoch: 143, Tr.Loss: 1.115466, Val.Loss: 1.174498, Tr.Acc: 0.560, Val.Acc: 0.525\n",
      "Epoch: 144, Tr.Loss: 1.115647, Val.Loss: 1.174101, Tr.Acc: 0.562, Val.Acc: 0.523\n",
      "Epoch: 145, Tr.Loss: 1.109564, Val.Loss: 1.169384, Tr.Acc: 0.563, Val.Acc: 0.526\n",
      "Epoch: 146, Tr.Loss: 1.109665, Val.Loss: 1.165865, Tr.Acc: 0.563, Val.Acc: 0.528\n",
      "Epoch: 147, Tr.Loss: 1.101749, Val.Loss: 1.162279, Tr.Acc: 0.570, Val.Acc: 0.534\n",
      "Epoch: 148, Tr.Loss: 1.097367, Val.Loss: 1.159470, Tr.Acc: 0.568, Val.Acc: 0.535\n",
      "Epoch: 149, Tr.Loss: 1.095700, Val.Loss: 1.156633, Tr.Acc: 0.571, Val.Acc: 0.537\n",
      "Epoch: 150, Tr.Loss: 1.088706, Val.Loss: 1.154277, Tr.Acc: 0.573, Val.Acc: 0.535\n",
      "Epoch: 151, Tr.Loss: 1.085947, Val.Loss: 1.151866, Tr.Acc: 0.572, Val.Acc: 0.541\n",
      "Epoch: 152, Tr.Loss: 1.080997, Val.Loss: 1.149740, Tr.Acc: 0.578, Val.Acc: 0.538\n",
      "Epoch: 153, Tr.Loss: 1.077797, Val.Loss: 1.145984, Tr.Acc: 0.578, Val.Acc: 0.542\n",
      "Epoch: 154, Tr.Loss: 1.084366, Val.Loss: 1.143493, Tr.Acc: 0.573, Val.Acc: 0.542\n",
      "Epoch: 155, Tr.Loss: 1.076867, Val.Loss: 1.139833, Tr.Acc: 0.579, Val.Acc: 0.544\n",
      "Epoch: 156, Tr.Loss: 1.067887, Val.Loss: 1.138333, Tr.Acc: 0.582, Val.Acc: 0.543\n",
      "Epoch: 157, Tr.Loss: 1.061367, Val.Loss: 1.137617, Tr.Acc: 0.584, Val.Acc: 0.545\n",
      "Epoch: 158, Tr.Loss: 1.064335, Val.Loss: 1.133754, Tr.Acc: 0.583, Val.Acc: 0.545\n",
      "Epoch: 159, Tr.Loss: 1.060622, Val.Loss: 1.132472, Tr.Acc: 0.585, Val.Acc: 0.548\n",
      "Epoch: 160, Tr.Loss: 1.059743, Val.Loss: 1.128602, Tr.Acc: 0.582, Val.Acc: 0.548\n",
      "Epoch: 161, Tr.Loss: 1.054527, Val.Loss: 1.126573, Tr.Acc: 0.586, Val.Acc: 0.547\n",
      "Epoch: 162, Tr.Loss: 1.055427, Val.Loss: 1.126168, Tr.Acc: 0.589, Val.Acc: 0.549\n",
      "Epoch: 163, Tr.Loss: 1.047579, Val.Loss: 1.121478, Tr.Acc: 0.591, Val.Acc: 0.551\n",
      "Epoch: 164, Tr.Loss: 1.043715, Val.Loss: 1.119607, Tr.Acc: 0.592, Val.Acc: 0.551\n",
      "Epoch: 165, Tr.Loss: 1.044888, Val.Loss: 1.118730, Tr.Acc: 0.589, Val.Acc: 0.553\n",
      "Epoch: 166, Tr.Loss: 1.037138, Val.Loss: 1.116124, Tr.Acc: 0.594, Val.Acc: 0.551\n",
      "Epoch: 167, Tr.Loss: 1.033233, Val.Loss: 1.114025, Tr.Acc: 0.597, Val.Acc: 0.553\n",
      "Epoch: 168, Tr.Loss: 1.026064, Val.Loss: 1.111010, Tr.Acc: 0.600, Val.Acc: 0.559\n",
      "Epoch: 169, Tr.Loss: 1.025349, Val.Loss: 1.109270, Tr.Acc: 0.601, Val.Acc: 0.557\n",
      "Epoch: 170, Tr.Loss: 1.022669, Val.Loss: 1.108745, Tr.Acc: 0.600, Val.Acc: 0.556\n",
      "Epoch: 171, Tr.Loss: 1.025895, Val.Loss: 1.107894, Tr.Acc: 0.597, Val.Acc: 0.553\n",
      "Epoch: 172, Tr.Loss: 1.013953, Val.Loss: 1.104930, Tr.Acc: 0.605, Val.Acc: 0.556\n",
      "Epoch: 173, Tr.Loss: 1.014128, Val.Loss: 1.105833, Tr.Acc: 0.605, Val.Acc: 0.555\n",
      "Epoch: 174, Tr.Loss: 1.009671, Val.Loss: 1.102215, Tr.Acc: 0.605, Val.Acc: 0.557\n",
      "Epoch: 175, Tr.Loss: 1.010583, Val.Loss: 1.100930, Tr.Acc: 0.605, Val.Acc: 0.558\n",
      "Epoch: 176, Tr.Loss: 1.009781, Val.Loss: 1.099668, Tr.Acc: 0.605, Val.Acc: 0.560\n",
      "Epoch: 177, Tr.Loss: 0.995851, Val.Loss: 1.097009, Tr.Acc: 0.614, Val.Acc: 0.559\n",
      "Epoch: 178, Tr.Loss: 0.998653, Val.Loss: 1.096047, Tr.Acc: 0.610, Val.Acc: 0.559\n",
      "Epoch: 179, Tr.Loss: 0.996602, Val.Loss: 1.094612, Tr.Acc: 0.610, Val.Acc: 0.559\n",
      "Epoch: 180, Tr.Loss: 0.990991, Val.Loss: 1.095401, Tr.Acc: 0.612, Val.Acc: 0.559\n",
      "Epoch: 181, Tr.Loss: 0.993388, Val.Loss: 1.092155, Tr.Acc: 0.614, Val.Acc: 0.563\n",
      "Epoch: 182, Tr.Loss: 0.992637, Val.Loss: 1.093856, Tr.Acc: 0.610, Val.Acc: 0.562\n",
      "Epoch: 183, Tr.Loss: 0.986027, Val.Loss: 1.090038, Tr.Acc: 0.615, Val.Acc: 0.564\n",
      "Epoch: 184, Tr.Loss: 0.981332, Val.Loss: 1.088506, Tr.Acc: 0.615, Val.Acc: 0.563\n",
      "Epoch: 185, Tr.Loss: 0.981652, Val.Loss: 1.090262, Tr.Acc: 0.617, Val.Acc: 0.559\n",
      "Epoch: 186, Tr.Loss: 0.976553, Val.Loss: 1.086200, Tr.Acc: 0.618, Val.Acc: 0.562\n",
      "Epoch: 187, Tr.Loss: 0.973871, Val.Loss: 1.086115, Tr.Acc: 0.621, Val.Acc: 0.563\n",
      "Epoch: 188, Tr.Loss: 0.977093, Val.Loss: 1.082933, Tr.Acc: 0.620, Val.Acc: 0.568\n",
      "Epoch: 189, Tr.Loss: 0.968172, Val.Loss: 1.084347, Tr.Acc: 0.621, Val.Acc: 0.563\n",
      "Epoch: 190, Tr.Loss: 0.973005, Val.Loss: 1.081224, Tr.Acc: 0.621, Val.Acc: 0.565\n",
      "Epoch: 191, Tr.Loss: 0.965072, Val.Loss: 1.081730, Tr.Acc: 0.621, Val.Acc: 0.565\n",
      "Epoch: 192, Tr.Loss: 0.963622, Val.Loss: 1.083837, Tr.Acc: 0.623, Val.Acc: 0.564\n",
      "Epoch: 193, Tr.Loss: 0.967462, Val.Loss: 1.080191, Tr.Acc: 0.624, Val.Acc: 0.566\n",
      "Epoch: 194, Tr.Loss: 0.956529, Val.Loss: 1.077426, Tr.Acc: 0.627, Val.Acc: 0.568\n",
      "Epoch: 195, Tr.Loss: 0.956919, Val.Loss: 1.077698, Tr.Acc: 0.627, Val.Acc: 0.564\n",
      "Epoch: 196, Tr.Loss: 0.953239, Val.Loss: 1.076730, Tr.Acc: 0.627, Val.Acc: 0.565\n",
      "Epoch: 197, Tr.Loss: 0.949094, Val.Loss: 1.075754, Tr.Acc: 0.631, Val.Acc: 0.568\n",
      "Epoch: 198, Tr.Loss: 0.945995, Val.Loss: 1.074816, Tr.Acc: 0.632, Val.Acc: 0.568\n",
      "Epoch: 199, Tr.Loss: 0.952889, Val.Loss: 1.072368, Tr.Acc: 0.629, Val.Acc: 0.573\n",
      "Epoch: 200, Tr.Loss: 0.941512, Val.Loss: 1.072452, Tr.Acc: 0.632, Val.Acc: 0.570\n",
      "Epoch: 201, Tr.Loss: 0.946782, Val.Loss: 1.071415, Tr.Acc: 0.632, Val.Acc: 0.572\n",
      "Epoch: 202, Tr.Loss: 0.940502, Val.Loss: 1.069952, Tr.Acc: 0.634, Val.Acc: 0.574\n",
      "Epoch: 203, Tr.Loss: 0.934282, Val.Loss: 1.069393, Tr.Acc: 0.636, Val.Acc: 0.573\n",
      "Epoch: 204, Tr.Loss: 0.934628, Val.Loss: 1.069664, Tr.Acc: 0.637, Val.Acc: 0.570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 205, Tr.Loss: 0.926874, Val.Loss: 1.068303, Tr.Acc: 0.640, Val.Acc: 0.573\n",
      "Epoch: 206, Tr.Loss: 0.925136, Val.Loss: 1.067546, Tr.Acc: 0.639, Val.Acc: 0.575\n",
      "Epoch: 207, Tr.Loss: 0.922460, Val.Loss: 1.066656, Tr.Acc: 0.642, Val.Acc: 0.569\n",
      "Epoch: 208, Tr.Loss: 0.926678, Val.Loss: 1.066184, Tr.Acc: 0.637, Val.Acc: 0.571\n",
      "Epoch: 209, Tr.Loss: 0.919291, Val.Loss: 1.066357, Tr.Acc: 0.644, Val.Acc: 0.573\n",
      "Epoch: 210, Tr.Loss: 0.914038, Val.Loss: 1.064437, Tr.Acc: 0.646, Val.Acc: 0.574\n",
      "Epoch: 211, Tr.Loss: 0.913319, Val.Loss: 1.064668, Tr.Acc: 0.643, Val.Acc: 0.574\n",
      "Epoch: 212, Tr.Loss: 0.908167, Val.Loss: 1.064020, Tr.Acc: 0.648, Val.Acc: 0.572\n",
      "Epoch: 213, Tr.Loss: 0.907983, Val.Loss: 1.060771, Tr.Acc: 0.646, Val.Acc: 0.578\n",
      "Epoch: 214, Tr.Loss: 0.908761, Val.Loss: 1.062699, Tr.Acc: 0.648, Val.Acc: 0.573\n",
      "Epoch: 215, Tr.Loss: 0.907842, Val.Loss: 1.063675, Tr.Acc: 0.648, Val.Acc: 0.574\n",
      "Epoch: 216, Tr.Loss: 0.898042, Val.Loss: 1.059246, Tr.Acc: 0.651, Val.Acc: 0.576\n",
      "Epoch: 217, Tr.Loss: 0.905609, Val.Loss: 1.060918, Tr.Acc: 0.648, Val.Acc: 0.573\n",
      "Epoch: 218, Tr.Loss: 0.893241, Val.Loss: 1.060493, Tr.Acc: 0.652, Val.Acc: 0.574\n",
      "Epoch: 219, Tr.Loss: 0.892331, Val.Loss: 1.059502, Tr.Acc: 0.651, Val.Acc: 0.573\n",
      "Epoch: 220, Tr.Loss: 0.886866, Val.Loss: 1.056601, Tr.Acc: 0.656, Val.Acc: 0.577\n",
      "Epoch: 221, Tr.Loss: 0.885004, Val.Loss: 1.056030, Tr.Acc: 0.656, Val.Acc: 0.578\n",
      "Epoch: 222, Tr.Loss: 0.888544, Val.Loss: 1.057881, Tr.Acc: 0.655, Val.Acc: 0.573\n",
      "Epoch: 223, Tr.Loss: 0.878182, Val.Loss: 1.056979, Tr.Acc: 0.660, Val.Acc: 0.575\n",
      "Epoch: 224, Tr.Loss: 0.876536, Val.Loss: 1.057167, Tr.Acc: 0.657, Val.Acc: 0.580\n",
      "Epoch: 225, Tr.Loss: 0.876458, Val.Loss: 1.057258, Tr.Acc: 0.657, Val.Acc: 0.579\n",
      "Epoch: 226, Tr.Loss: 0.874488, Val.Loss: 1.060007, Tr.Acc: 0.660, Val.Acc: 0.573\n",
      "Epoch: 227, Tr.Loss: 0.873134, Val.Loss: 1.056637, Tr.Acc: 0.665, Val.Acc: 0.576\n",
      "Epoch: 228, Tr.Loss: 0.867976, Val.Loss: 1.057374, Tr.Acc: 0.663, Val.Acc: 0.581\n",
      "Epoch: 229, Tr.Loss: 0.859387, Val.Loss: 1.053303, Tr.Acc: 0.668, Val.Acc: 0.579\n",
      "Epoch: 230, Tr.Loss: 0.866544, Val.Loss: 1.054312, Tr.Acc: 0.658, Val.Acc: 0.582\n",
      "Epoch: 231, Tr.Loss: 0.859918, Val.Loss: 1.056425, Tr.Acc: 0.664, Val.Acc: 0.581\n",
      "Epoch: 232, Tr.Loss: 0.857549, Val.Loss: 1.054684, Tr.Acc: 0.669, Val.Acc: 0.583\n",
      "Epoch: 233, Tr.Loss: 0.849225, Val.Loss: 1.052320, Tr.Acc: 0.670, Val.Acc: 0.582\n",
      "Epoch: 234, Tr.Loss: 0.845935, Val.Loss: 1.052733, Tr.Acc: 0.672, Val.Acc: 0.583\n",
      "Epoch: 235, Tr.Loss: 0.851831, Val.Loss: 1.053268, Tr.Acc: 0.667, Val.Acc: 0.582\n",
      "Epoch: 236, Tr.Loss: 0.848434, Val.Loss: 1.054634, Tr.Acc: 0.671, Val.Acc: 0.582\n",
      "Epoch: 237, Tr.Loss: 0.843721, Val.Loss: 1.052991, Tr.Acc: 0.671, Val.Acc: 0.582\n",
      "Epoch: 238, Tr.Loss: 0.838553, Val.Loss: 1.053334, Tr.Acc: 0.677, Val.Acc: 0.581\n",
      "Epoch: 239, Tr.Loss: 0.839619, Val.Loss: 1.050757, Tr.Acc: 0.675, Val.Acc: 0.584\n",
      "Epoch: 240, Tr.Loss: 0.837041, Val.Loss: 1.052535, Tr.Acc: 0.677, Val.Acc: 0.587\n",
      "Epoch: 241, Tr.Loss: 0.831900, Val.Loss: 1.053044, Tr.Acc: 0.679, Val.Acc: 0.580\n",
      "Epoch: 242, Tr.Loss: 0.828012, Val.Loss: 1.053231, Tr.Acc: 0.680, Val.Acc: 0.579\n",
      "Epoch: 243, Tr.Loss: 0.824932, Val.Loss: 1.053707, Tr.Acc: 0.681, Val.Acc: 0.585\n",
      "Epoch: 244, Tr.Loss: 0.822036, Val.Loss: 1.053643, Tr.Acc: 0.682, Val.Acc: 0.586\n",
      "Epoch: 245, Tr.Loss: 0.814406, Val.Loss: 1.053601, Tr.Acc: 0.686, Val.Acc: 0.580\n",
      "Epoch: 246, Tr.Loss: 0.821638, Val.Loss: 1.058532, Tr.Acc: 0.682, Val.Acc: 0.582\n",
      "Epoch: 247, Tr.Loss: 0.813292, Val.Loss: 1.052857, Tr.Acc: 0.685, Val.Acc: 0.584\n",
      "Epoch: 248, Tr.Loss: 0.817097, Val.Loss: 1.057049, Tr.Acc: 0.685, Val.Acc: 0.583\n",
      "Epoch: 249, Tr.Loss: 0.808928, Val.Loss: 1.055506, Tr.Acc: 0.689, Val.Acc: 0.584\n",
      "Epoch: 250, Tr.Loss: 0.801554, Val.Loss: 1.052912, Tr.Acc: 0.691, Val.Acc: 0.588\n",
      "\n",
      "Testing the model .... \n",
      "\n",
      "Test Loss: 1.028287, Test Accuracy: 0.608\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#filtering the data to include only the five classes\n",
    "classes = [b'cat', b'deer', b'dog', b'frog', b'horse']\n",
    "class0 = labelNames.index(classes[0])\n",
    "class1 = labelNames.index(classes[1])\n",
    "class2 = labelNames.index(classes[2])\n",
    "class3 = labelNames.index(classes[3])\n",
    "class4 = labelNames.index(classes[4])\n",
    "\n",
    "trainX = trainImages[(trainLabels == class0) | (trainLabels == class1) | (trainLabels == class2) | \n",
    "                     (trainLabels == class3) | (trainLabels == class4)]\n",
    "trainY = trainLabels[(trainLabels == class0) | (trainLabels == class1) | (trainLabels == class2) | \n",
    "                     (trainLabels == class3) | (trainLabels == class4)]\n",
    "trainY[trainY == class0] = 0\n",
    "trainY[trainY == class1] = 1\n",
    "trainY[trainY == class2] = 2\n",
    "trainY[trainY == class3] = 3\n",
    "trainY[trainY == class4] = 4\n",
    "\n",
    "testX = testImages[(testLabels == class0) | (testLabels == class1) | (testLabels == class2) | \n",
    "                   (testLabels == class3) | (testLabels == class4)]\n",
    "testY = testLabels[(testLabels == class0) | (testLabels == class1) | (testLabels == class2) | \n",
    "                   (testLabels == class3) | (testLabels == class4)]\n",
    "testY[testY == class0] = 0\n",
    "testY[testY == class1] = 1\n",
    "testY[testY == class2] = 2\n",
    "testY[testY == class3] = 3\n",
    "testY[testY == class4] = 4\n",
    "\n",
    "\n",
    "#extracting the HOG features\n",
    "trainX = np.array([hog(image, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                       transform_sqrt=True, block_norm=\"L1\") for image in trainX])\n",
    "testX = np.array([hog(image, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2), \n",
    "                       transform_sqrt=True, block_norm=\"L1\") for image in testX])\n",
    "\n",
    "\n",
    "#normalizing the data set\n",
    "mean = np.mean(trainX, axis=0, keepdims=True)\n",
    "std = np.std(trainX, axis=0, keepdims=True)\n",
    "trainX = (trainX - mean)/std\n",
    "testX = (testX - mean)/std\n",
    "\n",
    "\n",
    "#values of the hyperparameters\n",
    "SEED = 100\n",
    "VAL_SPLIT = 0.2\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 250\n",
    "LEARNING_RATE = 0.005\n",
    "\n",
    "\n",
    "#splitting the training set into training and validation set\n",
    "np.random.seed(SEED)\n",
    "indices = np.arange(len(trainX))\n",
    "np.random.shuffle(indices)\n",
    "valX = trainX[indices[:int(VAL_SPLIT*len(indices))]]\n",
    "valY = trainY[indices[:int(VAL_SPLIT*len(indices))]]\n",
    "trainX = trainX[indices[int(VAL_SPLIT*len(indices)):]]\n",
    "trainY = trainY[indices[int(VAL_SPLIT*len(indices)):]]\n",
    "\n",
    "\n",
    "#Model architecture\n",
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = Linear(288,512,LEARNING_RATE)\n",
    "        self.afunc1 = ReLU()\n",
    "        self.fc2 = Linear(512,256,LEARNING_RATE)\n",
    "        self.afunc2 = ReLU()\n",
    "        self.fc3 = Linear(256,64,LEARNING_RATE)\n",
    "        self.afunc3 = ReLU()\n",
    "        self.fc4 = Linear(64,16,LEARNING_RATE)\n",
    "        self.afunc4 = ReLU()\n",
    "        self.fc5 = Linear(16,5,LEARNING_RATE)\n",
    "        self.afunc5 = Softmax()\n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.afunc1.forward(x)\n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.afunc2.forward(x)\n",
    "        x = self.fc3.forward(x)\n",
    "        x = self.afunc3.forward(x)\n",
    "        x = self.fc4.forward(x)\n",
    "        x = self.afunc4.forward(x)\n",
    "        x = self.fc5.forward(x)\n",
    "        x = self.afunc5.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, gradients):\n",
    "        gradients = self.afunc5.backward(gradients)\n",
    "        gradients = self.fc5.backward(gradients)\n",
    "        gradients = self.afunc4.backward(gradients)\n",
    "        gradients = self.fc4.backward(gradients)\n",
    "        gradients = self.afunc3.backward(gradients)\n",
    "        gradients = self.fc3.backward(gradients)\n",
    "        gradients = self.afunc2.backward(gradients)\n",
    "        gradients = self.fc2.backward(gradients)\n",
    "        gradients = self.afunc1.backward(gradients)\n",
    "        gradients = self.fc1.backward(gradients)\n",
    "        return\n",
    "    \n",
    "    def step(self):\n",
    "        self.fc1.step()\n",
    "        self.fc2.step()\n",
    "        self.fc3.step()\n",
    "        self.fc4.step()\n",
    "        self.fc5.step()\n",
    "        return\n",
    "    \n",
    "    def num_params(self):\n",
    "        numParams = 0\n",
    "        numParams = numParams + self.fc1.num_params()\n",
    "        numParams = numParams + self.fc2.num_params()\n",
    "        numParams = numParams + self.fc3.num_params()\n",
    "        numParams = numParams + self.fc4.num_params()\n",
    "        numParams = numParams + self.fc5.num_params()\n",
    "        return numParams\n",
    "    \n",
    "    \n",
    "#loss function and model initialization\n",
    "loss_function = CrossEntropyLoss()\n",
    "model = Net()\n",
    "print(\"\\nNumber of Parameters in Model = \" + str(model.num_params()))\n",
    "\n",
    "\n",
    "print('\\nTraining the model .... \\n')\n",
    "trainingLossCurve = []\n",
    "validationLossCurve = []\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    \n",
    "    #shuffle the dataset so that the composition of batches differs every epoch\n",
    "    indices = np.arange(len(trainX))\n",
    "    np.random.shuffle(indices)\n",
    "    trainX = trainX[indices]\n",
    "    trainY = trainY[indices]\n",
    "    \n",
    "    #train the model for one epoch and obtain the training set loss and accuracy\n",
    "    trainingLoss, trainingAccuracy = train(model, trainX, trainY, loss_function)\n",
    "    trainingLossCurve.append(trainingLoss)\n",
    "    \n",
    "    #obtain the validation set loss and accuracy\n",
    "    validationLoss, validationAccuracy = validate(model, valX, valY, loss_function)\n",
    "    validationLossCurve.append(validationLoss)\n",
    "    print('Epoch: %d, Tr.Loss: %.6f, Val.Loss: %.6f, Tr.Acc: %.3f, Val.Acc: %.3f' \n",
    "          %(epoch, trainingLoss, validationLoss, trainingAccuracy, validationAccuracy))\n",
    "\n",
    "\n",
    "#obtain the test set loss and accuracy    \n",
    "print('\\nTesting the model .... \\n')\n",
    "testLoss, testAccuracy = test(model, testX, testY, loss_function)\n",
    "print('Test Loss: %.6f, Test Accuracy: %.3f' %(testLoss, testAccuracy))\n",
    "\n",
    "\n",
    "#plotting the training and validation loss curves\n",
    "plt.figure()\n",
    "plt.title('Loss Curves')\n",
    "plt.xlabel('Epoch No.')\n",
    "plt.ylabel('Loss value')\n",
    "plt.plot(trainingLossCurve, 'b', label='Train')\n",
    "plt.plot(validationLossCurve, 'r', label='Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "314px",
    "width": "464px"
   },
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "386.188px"
   },
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
